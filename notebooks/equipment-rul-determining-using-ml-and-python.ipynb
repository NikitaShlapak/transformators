{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "PATH = '../data/transformer-time/'"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-08-20T08:42:33.150336Z",
     "iopub.execute_input": "2023-08-20T08:42:33.150718Z",
     "iopub.status.idle": "2023-08-20T08:42:33.156821Z",
     "shell.execute_reply.started": "2023-08-20T08:42:33.150665Z",
     "shell.execute_reply": "2023-08-20T08:42:33.155526Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-08-20T14:01:37.108271865Z",
     "start_time": "2023-08-20T14:01:37.092414396Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Available (Train) set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "y_data = pd.read_csv(PATH + 'train.csv', index_col='id')\n",
    "y_data.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T08:42:36.815971Z",
     "iopub.execute_input": "2023-08-20T08:42:36.816349Z",
     "iopub.status.idle": "2023-08-20T08:42:36.866658Z",
     "shell.execute_reply.started": "2023-08-20T08:42:36.816321Z",
     "shell.execute_reply": "2023-08-20T08:42:36.865773Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-08-20T14:01:40.031021673Z",
     "start_time": "2023-08-20T14:01:39.736219889Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                  predicted\nid                         \n2_trans_497.csv         550\n2_trans_483.csv        1093\n2_trans_2396.csv        861\n2_trans_1847.csv       1093\n2_trans_2382.csv        488",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>predicted</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2_trans_497.csv</th>\n      <td>550</td>\n    </tr>\n    <tr>\n      <th>2_trans_483.csv</th>\n      <td>1093</td>\n    </tr>\n    <tr>\n      <th>2_trans_2396.csv</th>\n      <td>861</td>\n    </tr>\n    <tr>\n      <th>2_trans_1847.csv</th>\n      <td>1093</td>\n    </tr>\n    <tr>\n      <th>2_trans_2382.csv</th>\n      <td>488</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "X_data = {}\n",
    "for row in y_data.iterrows():\n",
    "    file_name = row[0]\n",
    "    path = PATH + f'data_train/data_train/{file_name}'\n",
    "    X_data[file_name] = pd.read_csv(path)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T08:42:42.415315Z",
     "iopub.execute_input": "2023-08-20T08:42:42.415667Z",
     "iopub.status.idle": "2023-08-20T08:42:59.830551Z",
     "shell.execute_reply.started": "2023-08-20T08:42:42.415642Z",
     "shell.execute_reply": "2023-08-20T08:42:59.829203Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-08-20T14:01:46.977439724Z",
     "start_time": "2023-08-20T14:01:45.480260247Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_data[file_name].shape"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T08:43:01.958541Z",
     "iopub.execute_input": "2023-08-20T08:43:01.959846Z",
     "iopub.status.idle": "2023-08-20T08:43:01.966752Z",
     "shell.execute_reply.started": "2023-08-20T08:43:01.959804Z",
     "shell.execute_reply": "2023-08-20T08:43:01.965555Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-08-20T14:01:50.563143223Z",
     "start_time": "2023-08-20T14:01:50.545694440Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(420, 4)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "X_data[file_name].head(2)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T08:43:03.559400Z",
     "iopub.execute_input": "2023-08-20T08:43:03.559757Z",
     "iopub.status.idle": "2023-08-20T08:43:03.571486Z",
     "shell.execute_reply.started": "2023-08-20T08:43:03.559731Z",
     "shell.execute_reply": "2023-08-20T08:43:03.570047Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "execution_count": 16,
     "output_type": "execute_result",
     "data": {
      "text/plain": "         H2        CO      C2H4      C2H2\n0  0.001545  0.024891  0.002929  0.000135\n1  0.001545  0.024891  0.002928  0.000135",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>H2</th>\n      <th>CO</th>\n      <th>C2H4</th>\n      <th>C2H2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.001545</td>\n      <td>0.024891</td>\n      <td>0.002929</td>\n      <td>0.000135</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001545</td>\n      <td>0.024891</td>\n      <td>0.002928</td>\n      <td>0.000135</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we don't have y_test, all the experments will be conducted using only train set. So, we will divide train set into train and validation sets. Validation set will be used as left out test set for results evaluation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, f1_score\n",
    "from catboost import CatBoostRegressor\n",
    "from tsfresh.feature_extraction import extract_features, MinimalFCParameters, ComprehensiveFCParameters\n",
    "from tsfresh.feature_selection import  select_features"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T08:43:05.325851Z",
     "iopub.execute_input": "2023-08-20T08:43:05.326261Z",
     "iopub.status.idle": "2023-08-20T08:43:05.332386Z",
     "shell.execute_reply.started": "2023-08-20T08:43:05.326222Z",
     "shell.execute_reply": "2023-08-20T08:43:05.331577Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline using mean value of RUL"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "y_train, y_val = train_test_split(y_data, test_size=0.25, random_state=1)\n",
    "print(f'Train set shape: {y_train.shape}')\n",
    "print(f'Test set shape: {y_val.shape}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T08:43:10.519961Z",
     "iopub.execute_input": "2023-08-20T08:43:10.520396Z",
     "iopub.status.idle": "2023-08-20T08:43:10.530565Z",
     "shell.execute_reply.started": "2023-08-20T08:43:10.520362Z",
     "shell.execute_reply": "2023-08-20T08:43:10.529242Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train set shape: (1575, 1)\nTest set shape: (525, 1)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "y_train_pred = [y_train.mean().values[0]] * len(y_train)\n",
    "y_val_pred = [y_train.mean().values[0]] * len(y_val)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T07:22:41.237574Z",
     "iopub.execute_input": "2023-08-20T07:22:41.238144Z",
     "iopub.status.idle": "2023-08-20T07:22:41.256475Z",
     "shell.execute_reply.started": "2023-08-20T07:22:41.238097Z",
     "shell.execute_reply": "2023-08-20T07:22:41.255182Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mae_train = round(mean_absolute_error(y_train, y_train_pred), 2)\n",
    "mae_val = round(mean_absolute_error(y_val, y_val_pred), 2)\n",
    "print(f'MAE on train set = {mae_train}')\n",
    "print(f'MAE on test set = {mae_val}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T07:22:42.885069Z",
     "iopub.execute_input": "2023-08-20T07:22:42.885479Z",
     "iopub.status.idle": "2023-08-20T07:22:42.896321Z",
     "shell.execute_reply.started": "2023-08-20T07:22:42.885450Z",
     "shell.execute_reply": "2023-08-20T07:22:42.895163Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "MAE on train set = 219.97\nMAE on test set = 218.38\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regression on aggregated parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Approach 1: simple aggregation + linear regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "y = y_data.copy()\n",
    "X = pd.concat([X_data[file].mean() for file in y_data.index], axis=1).T\n",
    "X.index = y.index\n",
    "X = X.add_suffix('_mean')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "StSc = StandardScaler()\n",
    "X_train_sc = StSc.fit_transform(X_train)\n",
    "X_val_sc = StSc.transform(X_val)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_sc, y_train)\n",
    "\n",
    "y_train_pred = lr.predict(X_train_sc)\n",
    "y_val_pred = lr.predict(X_val_sc)\n",
    "\n",
    "mae_train = round(mean_absolute_error(y_train, y_train_pred), 2)\n",
    "mae_val = round(mean_absolute_error(y_val, y_val_pred), 2)\n",
    "print(f'MAE on train set = {mae_train}')\n",
    "print(f'MAE on test set = {mae_val}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-24T15:48:28.17019Z",
     "iopub.execute_input": "2023-06-24T15:48:28.17055Z",
     "iopub.status.idle": "2023-06-24T15:48:29.527644Z",
     "shell.execute_reply.started": "2023-06-24T15:48:28.170522Z",
     "shell.execute_reply": "2023-06-24T15:48:29.526155Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Approach 2: simple aggregation + gradient boosting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "y = y_data.copy()\n",
    "X = pd.concat([X_data[file].mean() for file in y_data.index], axis=1).T\n",
    "X.index = y.index\n",
    "X = X.add_suffix('_mean')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "cbr = CatBoostRegressor(random_state=1, verbose=0)\n",
    "cbr.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = cbr.predict(X_train)\n",
    "y_val_pred = cbr.predict(X_val)\n",
    "\n",
    "mae_train = round(mean_absolute_error(y_train, y_train_pred), 2)\n",
    "mae_val = round(mean_absolute_error(y_val, y_val_pred), 2)\n",
    "print(f'MAE on train set = {mae_train}')\n",
    "print(f'MAE on test set = {mae_val}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-24T15:48:29.529511Z",
     "iopub.execute_input": "2023-06-24T15:48:29.530025Z",
     "iopub.status.idle": "2023-06-24T15:48:32.039779Z",
     "shell.execute_reply.started": "2023-06-24T15:48:29.529985Z",
     "shell.execute_reply": "2023-06-24T15:48:32.038987Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Approach 3: complex aggregation (TSFresh) + linear regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "X = pd.concat([X_data[file].assign(id=file) for file in y_data.index], axis=0, ignore_index=True)\n",
    "y = y_data.copy()\n",
    "\n",
    "settings = MinimalFCParameters()\n",
    "X = extract_features(X, \n",
    "                     column_id=\"id\", \n",
    "                     default_fc_parameters=settings).loc[y.index]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "StSc = StandardScaler()\n",
    "X_train_sc = StSc.fit_transform(X_train)\n",
    "X_val_sc = StSc.transform(X_val)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-24T15:48:32.042069Z",
     "iopub.execute_input": "2023-06-24T15:48:32.042916Z",
     "iopub.status.idle": "2023-06-24T15:48:36.307674Z",
     "shell.execute_reply.started": "2023-06-24T15:48:32.042868Z",
     "shell.execute_reply": "2023-06-24T15:48:36.306361Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_sc, y_train)\n",
    "\n",
    "y_train_pred = lr.predict(X_train_sc)\n",
    "y_val_pred = lr.predict(X_val_sc)\n",
    "\n",
    "mae_train = round(mean_absolute_error(y_train, y_train_pred), 2)\n",
    "mae_val = round(mean_absolute_error(y_val, y_val_pred), 2)\n",
    "print(f'MAE on train set = {mae_train}')\n",
    "print(f'MAE on test set = {mae_val}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-24T15:48:36.310113Z",
     "iopub.execute_input": "2023-06-24T15:48:36.310973Z",
     "iopub.status.idle": "2023-06-24T15:48:36.348006Z",
     "shell.execute_reply.started": "2023-06-24T15:48:36.310885Z",
     "shell.execute_reply": "2023-06-24T15:48:36.345587Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Approach 4: complex aggregation (TSFresh) + gradient boosting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "X = pd.concat([X_data[file].assign(id=file) for file in y_data.index], axis=0, ignore_index=True)\n",
    "y = y_data.copy()\n",
    "\n",
    "settings = MinimalFCParameters()\n",
    "X = extract_features(X, \n",
    "                     column_id=\"id\", \n",
    "                     default_fc_parameters=settings).loc[y.index]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T08:43:22.673661Z",
     "iopub.execute_input": "2023-08-20T08:43:22.674017Z",
     "iopub.status.idle": "2023-08-20T08:43:25.815142Z",
     "shell.execute_reply.started": "2023-08-20T08:43:22.673993Z",
     "shell.execute_reply": "2023-08-20T08:43:25.813753Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "text": "Feature Extraction: 100%|██████████| 10/10 [00:02<00:00,  4.51it/s]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cbr = CatBoostRegressor(random_state=1, verbose=0)\n",
    "cbr.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = cbr.predict(X_train)\n",
    "y_val_pred = cbr.predict(X_val)\n",
    "\n",
    "mae_train = round(mean_absolute_error(y_train, y_train_pred), 2)\n",
    "mae_val = round(mean_absolute_error(y_val, y_val_pred), 2)\n",
    "print(f'MAE on train set = {mae_train}')\n",
    "print(f'MAE on test set = {mae_val}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T07:32:36.913230Z",
     "iopub.execute_input": "2023-08-20T07:32:36.914432Z",
     "iopub.status.idle": "2023-08-20T07:32:46.628631Z",
     "shell.execute_reply.started": "2023-08-20T07:32:36.914378Z",
     "shell.execute_reply": "2023-08-20T07:32:46.627305Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "MAE on train set = 37.2\nMAE on test set = 91.37\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model is clearly overfitted."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Why is the error so big?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.hist(y_val_pred, bins=30, alpha=0.6, label='Predicted values on the test set')\n",
    "plt.hist(y_val, bins=30, alpha=0.6, label='True values of the test set')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-24T15:48:46.442193Z",
     "iopub.execute_input": "2023-06-24T15:48:46.442524Z",
     "iopub.status.idle": "2023-06-24T15:48:46.895817Z",
     "shell.execute_reply.started": "2023-06-24T15:48:46.442495Z",
     "shell.execute_reply": "2023-06-24T15:48:46.89426Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble: classification + regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stage 1: classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's call values equal to 1093 outliers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "y = (y_data == 1093).astype(int)\n",
    "X = pd.concat([X_data[file].assign(id=file) for file in y_data.index], axis=0, ignore_index=True)\n",
    "\n",
    "settings = ComprehensiveFCParameters()\n",
    "X = extract_features(X, \n",
    "                     column_id=\"id\", \n",
    "                     default_fc_parameters=settings).loc[y.index]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T08:43:34.007516Z",
     "iopub.execute_input": "2023-08-20T08:43:34.007907Z",
     "iopub.status.idle": "2023-08-20T08:59:10.627552Z",
     "shell.execute_reply.started": "2023-08-20T08:43:34.007875Z",
     "shell.execute_reply": "2023-08-20T08:59:10.626521Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "text": "Feature Extraction: 100%|██████████| 10/10 [15:29<00:00, 92.99s/it]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "X_train"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T09:04:44.338004Z",
     "iopub.execute_input": "2023-08-20T09:04:44.338377Z",
     "iopub.status.idle": "2023-08-20T09:04:44.372546Z",
     "shell.execute_reply.started": "2023-08-20T09:04:44.338348Z",
     "shell.execute_reply": "2023-08-20T09:04:44.371637Z"
    },
    "trusted": true
   },
   "execution_count": 37,
   "outputs": [
    {
     "execution_count": 37,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                  H2__variance_larger_than_standard_deviation  \\\nid                                                              \n2_trans_2001.csv                                          0.0   \n2_trans_2998.csv                                          0.0   \n2_trans_437.csv                                           0.0   \n2_trans_642.csv                                           0.0   \n2_trans_3000.csv                                          0.0   \n...                                                       ...   \n2_trans_41.csv                                            0.0   \n2_trans_975.csv                                           0.0   \n2_trans_2303.csv                                          0.0   \n2_trans_1460.csv                                          0.0   \n2_trans_54.csv                                            0.0   \n\n                  H2__has_duplicate_max  H2__has_duplicate_min  \\\nid                                                               \n2_trans_2001.csv                    0.0                    0.0   \n2_trans_2998.csv                    0.0                    0.0   \n2_trans_437.csv                     0.0                    0.0   \n2_trans_642.csv                     0.0                    0.0   \n2_trans_3000.csv                    0.0                    0.0   \n...                                 ...                    ...   \n2_trans_41.csv                      0.0                    0.0   \n2_trans_975.csv                     0.0                    0.0   \n2_trans_2303.csv                    0.0                    0.0   \n2_trans_1460.csv                    0.0                    0.0   \n2_trans_54.csv                      0.0                    0.0   \n\n                  H2__has_duplicate  H2__sum_values  H2__abs_energy  \\\nid                                                                    \n2_trans_2001.csv                0.0        0.994863        0.002362   \n2_trans_2998.csv                0.0        0.720606        0.001245   \n2_trans_437.csv                 0.0        0.233862        0.000132   \n2_trans_642.csv                 0.0        0.328945        0.000260   \n2_trans_3000.csv                0.0        0.775452        0.001442   \n...                             ...             ...             ...   \n2_trans_41.csv                  0.0        0.961194        0.002299   \n2_trans_975.csv                 0.0        0.448076        0.000480   \n2_trans_2303.csv                0.0        0.441261        0.000497   \n2_trans_1460.csv                0.0        0.092162        0.000023   \n2_trans_54.csv                  0.0        0.275026        0.000222   \n\n                  H2__mean_abs_change  H2__mean_change  \\\nid                                                       \n2_trans_2001.csv         1.331274e-06     1.243474e-06   \n2_trans_2998.csv         1.477622e-06     1.477622e-06   \n2_trans_437.csv          1.170104e-06     4.929081e-07   \n2_trans_642.csv          1.134916e-06     8.056558e-07   \n2_trans_3000.csv         2.017160e-06     1.754019e-06   \n...                               ...              ...   \n2_trans_41.csv           4.155758e-06     4.155758e-06   \n2_trans_975.csv          9.668378e-07     7.986838e-07   \n2_trans_2303.csv         3.329145e-06     2.687629e-06   \n2_trans_1460.csv         1.705714e-06     8.193427e-07   \n2_trans_54.csv           2.855368e-06     2.833298e-06   \n\n                  H2__mean_second_derivative_central  H2__median  ...  \\\nid                                                                ...   \n2_trans_2001.csv                        3.408576e-09    0.002344  ...   \n2_trans_2998.csv                        5.020607e-09    0.001718  ...   \n2_trans_437.csv                         4.312393e-09    0.000560  ...   \n2_trans_642.csv                         1.796542e-09    0.000770  ...   \n2_trans_3000.csv                        7.783435e-09    0.001854  ...   \n...                                              ...         ...  ...   \n2_trans_41.csv                          1.082169e-08    0.002131  ...   \n2_trans_975.csv                         2.188927e-09    0.001063  ...   \n2_trans_2303.csv                        6.575940e-09    0.000943  ...   \n2_trans_1460.csv                        3.774336e-09    0.000221  ...   \n2_trans_54.csv                          1.019257e-08    0.000563  ...   \n\n                  C2H2__fourier_entropy__bins_5  \\\nid                                                \n2_trans_2001.csv                       0.090729   \n2_trans_2998.csv                       0.136002   \n2_trans_437.csv                        0.045395   \n2_trans_642.csv                        0.090729   \n2_trans_3000.csv                       0.045395   \n...                                         ...   \n2_trans_41.csv                         0.125256   \n2_trans_975.csv                        0.136002   \n2_trans_2303.csv                       0.090729   \n2_trans_1460.csv                       0.090729   \n2_trans_54.csv                         0.045395   \n\n                  C2H2__fourier_entropy__bins_10  \\\nid                                                 \n2_trans_2001.csv                        0.136002   \n2_trans_2998.csv                        0.181214   \n2_trans_437.csv                         0.045395   \n2_trans_642.csv                         0.090729   \n2_trans_3000.csv                        0.125256   \n...                                          ...   \n2_trans_41.csv                          0.136002   \n2_trans_975.csv                         0.136002   \n2_trans_2303.csv                        0.136002   \n2_trans_1460.csv                        0.136002   \n2_trans_54.csv                          0.090729   \n\n                  C2H2__fourier_entropy__bins_100  \\\nid                                                  \n2_trans_2001.csv                         0.136002   \n2_trans_2998.csv                         0.271451   \n2_trans_437.csv                          0.181214   \n2_trans_642.csv                          0.136002   \n2_trans_3000.csv                         0.136002   \n...                                           ...   \n2_trans_41.csv                           0.181214   \n2_trans_975.csv                          0.136002   \n2_trans_2303.csv                         0.136002   \n2_trans_1460.csv                         0.136002   \n2_trans_54.csv                           0.170467   \n\n                  C2H2__permutation_entropy__dimension_3__tau_1  \\\nid                                                                \n2_trans_2001.csv                                       0.624938   \n2_trans_2998.csv                                       0.751863   \n2_trans_437.csv                                        0.118661   \n2_trans_642.csv                                        0.608520   \n2_trans_3000.csv                                       0.635809   \n...                                                         ...   \n2_trans_41.csv                                         0.635809   \n2_trans_975.csv                                        0.703053   \n2_trans_2303.csv                                       0.704964   \n2_trans_1460.csv                                       0.705265   \n2_trans_54.csv                                         0.332299   \n\n                  C2H2__permutation_entropy__dimension_4__tau_1  \\\nid                                                                \n2_trans_2001.csv                                       0.655497   \n2_trans_2998.csv                                       0.816460   \n2_trans_437.csv                                        0.142589   \n2_trans_642.csv                                        0.639001   \n2_trans_3000.csv                                       0.666413   \n...                                                         ...   \n2_trans_41.csv                                         0.666413   \n2_trans_975.csv                                        0.733778   \n2_trans_2303.csv                                       0.735684   \n2_trans_1460.csv                                       0.750476   \n2_trans_54.csv                                         0.360513   \n\n                  C2H2__permutation_entropy__dimension_5__tau_1  \\\nid                                                                \n2_trans_2001.csv                                       0.686154   \n2_trans_2998.csv                                       0.881158   \n2_trans_437.csv                                        0.166211   \n2_trans_642.csv                                        0.669582   \n2_trans_3000.csv                                       0.697117   \n...                                                         ...   \n2_trans_41.csv                                         0.697117   \n2_trans_975.csv                                        0.764604   \n2_trans_2303.csv                                       0.766505   \n2_trans_1460.csv                                       0.795811   \n2_trans_54.csv                                         0.388776   \n\n                  C2H2__permutation_entropy__dimension_6__tau_1  \\\nid                                                                \n2_trans_2001.csv                                       0.716912   \n2_trans_2998.csv                                       0.945955   \n2_trans_437.csv                                        0.189443   \n2_trans_642.csv                                        0.700261   \n2_trans_3000.csv                                       0.727921   \n...                                                         ...   \n2_trans_41.csv                                         0.727921   \n2_trans_975.csv                                        0.795531   \n2_trans_2303.csv                                       0.797426   \n2_trans_1460.csv                                       0.841267   \n2_trans_54.csv                                         0.417086   \n\n                  C2H2__permutation_entropy__dimension_7__tau_1  \\\nid                                                                \n2_trans_2001.csv                                       0.747770   \n2_trans_2998.csv                                       1.014197   \n2_trans_437.csv                                        0.212160   \n2_trans_642.csv                                        0.731039   \n2_trans_3000.csv                                       0.758826   \n...                                                         ...   \n2_trans_41.csv                                         0.758826   \n2_trans_975.csv                                        0.826558   \n2_trans_2303.csv                                       0.828447   \n2_trans_1460.csv                                       0.886846   \n2_trans_54.csv                                         0.445442   \n\n                  C2H2__query_similarity_count__query_None__threshold_0.0  \\\nid                                                                          \n2_trans_2001.csv                                                NaN         \n2_trans_2998.csv                                                NaN         \n2_trans_437.csv                                                 NaN         \n2_trans_642.csv                                                 NaN         \n2_trans_3000.csv                                                NaN         \n...                                                             ...         \n2_trans_41.csv                                                  NaN         \n2_trans_975.csv                                                 NaN         \n2_trans_2303.csv                                                NaN         \n2_trans_1460.csv                                                NaN         \n2_trans_54.csv                                                  NaN         \n\n                  C2H2__mean_n_absolute_max__number_of_maxima_7  \nid                                                               \n2_trans_2001.csv                                       0.000390  \n2_trans_2998.csv                                       0.000027  \n2_trans_437.csv                                        0.000379  \n2_trans_642.csv                                        0.000387  \n2_trans_3000.csv                                       0.000057  \n...                                                         ...  \n2_trans_41.csv                                         0.000226  \n2_trans_975.csv                                        0.000065  \n2_trans_2303.csv                                       0.000226  \n2_trans_1460.csv                                       0.000148  \n2_trans_54.csv                                         0.000403  \n\n[1575 rows x 3132 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>H2__variance_larger_than_standard_deviation</th>\n      <th>H2__has_duplicate_max</th>\n      <th>H2__has_duplicate_min</th>\n      <th>H2__has_duplicate</th>\n      <th>H2__sum_values</th>\n      <th>H2__abs_energy</th>\n      <th>H2__mean_abs_change</th>\n      <th>H2__mean_change</th>\n      <th>H2__mean_second_derivative_central</th>\n      <th>H2__median</th>\n      <th>...</th>\n      <th>C2H2__fourier_entropy__bins_5</th>\n      <th>C2H2__fourier_entropy__bins_10</th>\n      <th>C2H2__fourier_entropy__bins_100</th>\n      <th>C2H2__permutation_entropy__dimension_3__tau_1</th>\n      <th>C2H2__permutation_entropy__dimension_4__tau_1</th>\n      <th>C2H2__permutation_entropy__dimension_5__tau_1</th>\n      <th>C2H2__permutation_entropy__dimension_6__tau_1</th>\n      <th>C2H2__permutation_entropy__dimension_7__tau_1</th>\n      <th>C2H2__query_similarity_count__query_None__threshold_0.0</th>\n      <th>C2H2__mean_n_absolute_max__number_of_maxima_7</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2_trans_2001.csv</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.994863</td>\n      <td>0.002362</td>\n      <td>1.331274e-06</td>\n      <td>1.243474e-06</td>\n      <td>3.408576e-09</td>\n      <td>0.002344</td>\n      <td>...</td>\n      <td>0.090729</td>\n      <td>0.136002</td>\n      <td>0.136002</td>\n      <td>0.624938</td>\n      <td>0.655497</td>\n      <td>0.686154</td>\n      <td>0.716912</td>\n      <td>0.747770</td>\n      <td>NaN</td>\n      <td>0.000390</td>\n    </tr>\n    <tr>\n      <th>2_trans_2998.csv</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.720606</td>\n      <td>0.001245</td>\n      <td>1.477622e-06</td>\n      <td>1.477622e-06</td>\n      <td>5.020607e-09</td>\n      <td>0.001718</td>\n      <td>...</td>\n      <td>0.136002</td>\n      <td>0.181214</td>\n      <td>0.271451</td>\n      <td>0.751863</td>\n      <td>0.816460</td>\n      <td>0.881158</td>\n      <td>0.945955</td>\n      <td>1.014197</td>\n      <td>NaN</td>\n      <td>0.000027</td>\n    </tr>\n    <tr>\n      <th>2_trans_437.csv</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.233862</td>\n      <td>0.000132</td>\n      <td>1.170104e-06</td>\n      <td>4.929081e-07</td>\n      <td>4.312393e-09</td>\n      <td>0.000560</td>\n      <td>...</td>\n      <td>0.045395</td>\n      <td>0.045395</td>\n      <td>0.181214</td>\n      <td>0.118661</td>\n      <td>0.142589</td>\n      <td>0.166211</td>\n      <td>0.189443</td>\n      <td>0.212160</td>\n      <td>NaN</td>\n      <td>0.000379</td>\n    </tr>\n    <tr>\n      <th>2_trans_642.csv</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.328945</td>\n      <td>0.000260</td>\n      <td>1.134916e-06</td>\n      <td>8.056558e-07</td>\n      <td>1.796542e-09</td>\n      <td>0.000770</td>\n      <td>...</td>\n      <td>0.090729</td>\n      <td>0.090729</td>\n      <td>0.136002</td>\n      <td>0.608520</td>\n      <td>0.639001</td>\n      <td>0.669582</td>\n      <td>0.700261</td>\n      <td>0.731039</td>\n      <td>NaN</td>\n      <td>0.000387</td>\n    </tr>\n    <tr>\n      <th>2_trans_3000.csv</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.775452</td>\n      <td>0.001442</td>\n      <td>2.017160e-06</td>\n      <td>1.754019e-06</td>\n      <td>7.783435e-09</td>\n      <td>0.001854</td>\n      <td>...</td>\n      <td>0.045395</td>\n      <td>0.125256</td>\n      <td>0.136002</td>\n      <td>0.635809</td>\n      <td>0.666413</td>\n      <td>0.697117</td>\n      <td>0.727921</td>\n      <td>0.758826</td>\n      <td>NaN</td>\n      <td>0.000057</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2_trans_41.csv</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.961194</td>\n      <td>0.002299</td>\n      <td>4.155758e-06</td>\n      <td>4.155758e-06</td>\n      <td>1.082169e-08</td>\n      <td>0.002131</td>\n      <td>...</td>\n      <td>0.125256</td>\n      <td>0.136002</td>\n      <td>0.181214</td>\n      <td>0.635809</td>\n      <td>0.666413</td>\n      <td>0.697117</td>\n      <td>0.727921</td>\n      <td>0.758826</td>\n      <td>NaN</td>\n      <td>0.000226</td>\n    </tr>\n    <tr>\n      <th>2_trans_975.csv</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.448076</td>\n      <td>0.000480</td>\n      <td>9.668378e-07</td>\n      <td>7.986838e-07</td>\n      <td>2.188927e-09</td>\n      <td>0.001063</td>\n      <td>...</td>\n      <td>0.136002</td>\n      <td>0.136002</td>\n      <td>0.136002</td>\n      <td>0.703053</td>\n      <td>0.733778</td>\n      <td>0.764604</td>\n      <td>0.795531</td>\n      <td>0.826558</td>\n      <td>NaN</td>\n      <td>0.000065</td>\n    </tr>\n    <tr>\n      <th>2_trans_2303.csv</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.441261</td>\n      <td>0.000497</td>\n      <td>3.329145e-06</td>\n      <td>2.687629e-06</td>\n      <td>6.575940e-09</td>\n      <td>0.000943</td>\n      <td>...</td>\n      <td>0.090729</td>\n      <td>0.136002</td>\n      <td>0.136002</td>\n      <td>0.704964</td>\n      <td>0.735684</td>\n      <td>0.766505</td>\n      <td>0.797426</td>\n      <td>0.828447</td>\n      <td>NaN</td>\n      <td>0.000226</td>\n    </tr>\n    <tr>\n      <th>2_trans_1460.csv</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.092162</td>\n      <td>0.000023</td>\n      <td>1.705714e-06</td>\n      <td>8.193427e-07</td>\n      <td>3.774336e-09</td>\n      <td>0.000221</td>\n      <td>...</td>\n      <td>0.090729</td>\n      <td>0.136002</td>\n      <td>0.136002</td>\n      <td>0.705265</td>\n      <td>0.750476</td>\n      <td>0.795811</td>\n      <td>0.841267</td>\n      <td>0.886846</td>\n      <td>NaN</td>\n      <td>0.000148</td>\n    </tr>\n    <tr>\n      <th>2_trans_54.csv</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.275026</td>\n      <td>0.000222</td>\n      <td>2.855368e-06</td>\n      <td>2.833298e-06</td>\n      <td>1.019257e-08</td>\n      <td>0.000563</td>\n      <td>...</td>\n      <td>0.045395</td>\n      <td>0.090729</td>\n      <td>0.170467</td>\n      <td>0.332299</td>\n      <td>0.360513</td>\n      <td>0.388776</td>\n      <td>0.417086</td>\n      <td>0.445442</td>\n      <td>NaN</td>\n      <td>0.000403</td>\n    </tr>\n  </tbody>\n</table>\n<p>1575 rows × 3132 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "X_train_filtered = select_features(X_train.dropna(axis=1), pd.Series(y_train['predicted']))\n",
    "relevant_features = set(X_train_filtered.columns)\n",
    "\n",
    "X_train_filtered = X_train[list(relevant_features)]\n",
    "X_val_filtered = X_val[list(relevant_features)]\n",
    "\n",
    "X_train_filtered"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T09:04:17.843389Z",
     "iopub.execute_input": "2023-08-20T09:04:17.843793Z",
     "iopub.status.idle": "2023-08-20T09:04:31.284311Z",
     "shell.execute_reply.started": "2023-08-20T09:04:17.843764Z",
     "shell.execute_reply": "2023-08-20T09:04:31.282762Z"
    },
    "trusted": true
   },
   "execution_count": 36,
   "outputs": [
    {
     "execution_count": 36,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                  C2H2__fft_coefficient__attr_\"real\"__coeff_51  \\\nid                                                               \n2_trans_2001.csv                                     -0.000076   \n2_trans_2998.csv                                     -0.000008   \n2_trans_437.csv                                      -0.000025   \n2_trans_642.csv                                      -0.000071   \n2_trans_3000.csv                                     -0.000012   \n...                                                        ...   \n2_trans_41.csv                                       -0.000023   \n2_trans_975.csv                                      -0.000018   \n2_trans_2303.csv                                     -0.000022   \n2_trans_1460.csv                                     -0.000071   \n2_trans_54.csv                                       -0.000038   \n\n                  C2H4__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.6__ql_0.4  \\\nid                                                                                    \n2_trans_2001.csv                                       1.753367e-06                   \n2_trans_2998.csv                                       7.463647e-07                   \n2_trans_437.csv                                        3.602815e-07                   \n2_trans_642.csv                                        2.613900e-06                   \n2_trans_3000.csv                                       1.689256e-06                   \n...                                                             ...                   \n2_trans_41.csv                                         1.315060e-06                   \n2_trans_975.csv                                        3.071787e-07                   \n2_trans_2303.csv                                       1.486050e-06                   \n2_trans_1460.csv                                       1.720797e-06                   \n2_trans_54.csv                                         6.544150e-07                   \n\n                  C2H2__fft_coefficient__attr_\"real\"__coeff_22  \\\nid                                                               \n2_trans_2001.csv                                     -0.000066   \n2_trans_2998.csv                                     -0.000001   \n2_trans_437.csv                                      -0.000023   \n2_trans_642.csv                                      -0.000065   \n2_trans_3000.csv                                     -0.000010   \n...                                                        ...   \n2_trans_41.csv                                       -0.000019   \n2_trans_975.csv                                      -0.000014   \n2_trans_2303.csv                                     -0.000022   \n2_trans_1460.csv                                     -0.000060   \n2_trans_54.csv                                       -0.000035   \n\n                  CO__fft_coefficient__attr_\"angle\"__coeff_89  \\\nid                                                              \n2_trans_2001.csv                                   127.797766   \n2_trans_2998.csv                                   -50.449042   \n2_trans_437.csv                                    127.806625   \n2_trans_642.csv                                    127.947039   \n2_trans_3000.csv                                   127.195458   \n...                                                       ...   \n2_trans_41.csv                                     127.954656   \n2_trans_975.csv                                    127.811576   \n2_trans_2303.csv                                   127.968230   \n2_trans_1460.csv                                   127.929090   \n2_trans_54.csv                                     127.903069   \n\n                  H2__fft_coefficient__attr_\"abs\"__coeff_58  \\\nid                                                            \n2_trans_2001.csv                                   0.000622   \n2_trans_2998.csv                                   0.000739   \n2_trans_437.csv                                    0.000247   \n2_trans_642.csv                                    0.000404   \n2_trans_3000.csv                                   0.000878   \n...                                                     ...   \n2_trans_41.csv                                     0.002077   \n2_trans_975.csv                                    0.000400   \n2_trans_2303.csv                                   0.001346   \n2_trans_1460.csv                                   0.000412   \n2_trans_54.csv                                     0.001417   \n\n                  H2__fft_coefficient__attr_\"abs\"__coeff_21  \\\nid                                                            \n2_trans_2001.csv                                   0.001674   \n2_trans_2998.csv                                   0.001986   \n2_trans_437.csv                                    0.000665   \n2_trans_642.csv                                    0.001087   \n2_trans_3000.csv                                   0.002359   \n...                                                     ...   \n2_trans_41.csv                                     0.005582   \n2_trans_975.csv                                    0.001076   \n2_trans_2303.csv                                   0.003620   \n2_trans_1460.csv                                   0.001109   \n2_trans_54.csv                                     0.003807   \n\n                  CO__agg_linear_trend__attr_\"rvalue\"__chunk_len_10__f_agg_\"min\"  \\\nid                                                                                 \n2_trans_2001.csv                                           0.709943                \n2_trans_2998.csv                                          -0.396254                \n2_trans_437.csv                                            0.856590                \n2_trans_642.csv                                            0.850631                \n2_trans_3000.csv                                           0.412096                \n...                                                             ...                \n2_trans_41.csv                                             0.962846                \n2_trans_975.csv                                            0.301402                \n2_trans_2303.csv                                           0.574719                \n2_trans_1460.csv                                           0.911751                \n2_trans_54.csv                                             0.966420                \n\n                  CO__fft_coefficient__attr_\"imag\"__coeff_61  \\\nid                                                             \n2_trans_2001.csv                                    0.006630   \n2_trans_2998.csv                                   -0.000372   \n2_trans_437.csv                                     0.008115   \n2_trans_642.csv                                     0.021821   \n2_trans_3000.csv                                    0.000681   \n...                                                      ...   \n2_trans_41.csv                                      0.008649   \n2_trans_975.csv                                     0.004073   \n2_trans_2303.csv                                    0.006436   \n2_trans_1460.csv                                    0.005491   \n2_trans_54.csv                                      0.006737   \n\n                  C2H4__ar_coefficient__coeff_3__k_10  \\\nid                                                      \n2_trans_2001.csv                             3.618489   \n2_trans_2998.csv                            -0.957440   \n2_trans_437.csv                              3.484438   \n2_trans_642.csv                              2.471616   \n2_trans_3000.csv                            -1.444416   \n...                                               ...   \n2_trans_41.csv                               3.510461   \n2_trans_975.csv                              2.757969   \n2_trans_2303.csv                             2.723769   \n2_trans_1460.csv                             3.047808   \n2_trans_54.csv                               3.438872   \n\n                  C2H4__fft_coefficient__attr_\"imag\"__coeff_19  ...  \\\nid                                                              ...   \n2_trans_2001.csv                                      0.004326  ...   \n2_trans_2998.csv                                      0.001282  ...   \n2_trans_437.csv                                       0.000406  ...   \n2_trans_642.csv                                       0.016747  ...   \n2_trans_3000.csv                                      0.001594  ...   \n...                                                        ...  ...   \n2_trans_41.csv                                        0.007937  ...   \n2_trans_975.csv                                       0.006058  ...   \n2_trans_2303.csv                                      0.007123  ...   \n2_trans_1460.csv                                      0.016432  ...   \n2_trans_54.csv                                        0.002730  ...   \n\n                  CO__cwt_coefficients__coeff_1__w_2__widths_(2, 5, 10, 20)  \\\nid                                                                            \n2_trans_2001.csv                                           0.001976           \n2_trans_2998.csv                                           0.000461           \n2_trans_437.csv                                            0.001508           \n2_trans_642.csv                                            0.006027           \n2_trans_3000.csv                                           0.000154           \n...                                                             ...           \n2_trans_41.csv                                             0.015438           \n2_trans_975.csv                                            0.003964           \n2_trans_2303.csv                                           0.014063           \n2_trans_1460.csv                                           0.014844           \n2_trans_54.csv                                             0.004812           \n\n                  C2H4__symmetry_looking__r_0.1  \\\nid                                                \n2_trans_2001.csv                            1.0   \n2_trans_2998.csv                            1.0   \n2_trans_437.csv                             1.0   \n2_trans_642.csv                             0.0   \n2_trans_3000.csv                            1.0   \n...                                         ...   \n2_trans_41.csv                              1.0   \n2_trans_975.csv                             1.0   \n2_trans_2303.csv                            0.0   \n2_trans_1460.csv                            0.0   \n2_trans_54.csv                              1.0   \n\n                  C2H4__fft_coefficient__attr_\"imag\"__coeff_25  \\\nid                                                               \n2_trans_2001.csv                                      0.003271   \n2_trans_2998.csv                                      0.000972   \n2_trans_437.csv                                       0.000307   \n2_trans_642.csv                                       0.012657   \n2_trans_3000.csv                                      0.001207   \n...                                                        ...   \n2_trans_41.csv                                        0.006004   \n2_trans_975.csv                                       0.004580   \n2_trans_2303.csv                                      0.005383   \n2_trans_1460.csv                                      0.012421   \n2_trans_54.csv                                        0.002065   \n\n                  C2H4__cwt_coefficients__coeff_5__w_10__widths_(2, 5, 10, 20)  \\\nid                                                                               \n2_trans_2001.csv                                           0.000419              \n2_trans_2998.csv                                           0.000363              \n2_trans_437.csv                                            0.001083              \n2_trans_642.csv                                            0.008711              \n2_trans_3000.csv                                           0.000008              \n...                                                             ...              \n2_trans_41.csv                                             0.006966              \n2_trans_975.csv                                            0.003229              \n2_trans_2303.csv                                           0.001727              \n2_trans_1460.csv                                           0.000039              \n2_trans_54.csv                                             0.009053              \n\n                  CO__cwt_coefficients__coeff_5__w_5__widths_(2, 5, 10, 20)  \\\nid                                                                            \n2_trans_2001.csv                                           0.004151           \n2_trans_2998.csv                                           0.000974           \n2_trans_437.csv                                            0.003186           \n2_trans_642.csv                                            0.012729           \n2_trans_3000.csv                                           0.000337           \n...                                                             ...           \n2_trans_41.csv                                             0.032685           \n2_trans_975.csv                                            0.008339           \n2_trans_2303.csv                                           0.029762           \n2_trans_1460.csv                                           0.031416           \n2_trans_54.csv                                             0.010185           \n\n                  CO__fft_coefficient__attr_\"abs\"__coeff_51  \\\nid                                                            \n2_trans_2001.csv                                   0.008701   \n2_trans_2998.csv                                   0.000501   \n2_trans_437.csv                                    0.010649   \n2_trans_642.csv                                    0.028690   \n2_trans_3000.csv                                   0.001097   \n...                                                     ...   \n2_trans_41.csv                                     0.011374   \n2_trans_975.csv                                    0.005347   \n2_trans_2303.csv                                   0.008466   \n2_trans_1460.csv                                   0.007217   \n2_trans_54.csv                                     0.008852   \n\n                  C2H4__cwt_coefficients__coeff_1__w_5__widths_(2, 5, 10, 20)  \\\nid                                                                              \n2_trans_2001.csv                                           0.000140             \n2_trans_2998.csv                                           0.000116             \n2_trans_437.csv                                            0.000346             \n2_trans_642.csv                                            0.002743             \n2_trans_3000.csv                                           0.000007             \n...                                                             ...             \n2_trans_41.csv                                             0.002203             \n2_trans_975.csv                                            0.001020             \n2_trans_2303.csv                                           0.000536             \n2_trans_1460.csv                                           0.000021             \n2_trans_54.csv                                             0.002856             \n\n                  C2H2__lempel_ziv_complexity__bins_5  \\\nid                                                      \n2_trans_2001.csv                             0.128571   \n2_trans_2998.csv                             0.150000   \n2_trans_437.csv                              0.145238   \n2_trans_642.csv                              0.130952   \n2_trans_3000.csv                             0.150000   \n...                                               ...   \n2_trans_41.csv                               0.133333   \n2_trans_975.csv                              0.145238   \n2_trans_2303.csv                             0.145238   \n2_trans_1460.csv                             0.128571   \n2_trans_54.csv                               0.147619   \n\n                  CO__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.8__ql_0.6  \\\nid                                                                                \n2_trans_2001.csv                                       1.969936e-10               \n2_trans_2998.csv                                       2.943249e-11               \n2_trans_437.csv                                        1.345016e-10               \n2_trans_642.csv                                        2.248016e-10               \n2_trans_3000.csv                                       1.669509e-11               \n...                                                             ...               \n2_trans_41.csv                                         5.262857e-11               \n2_trans_975.csv                                        7.059611e-11               \n2_trans_2303.csv                                       2.848821e-10               \n2_trans_1460.csv                                       2.426484e-11               \n2_trans_54.csv                                         4.913253e-11               \n\n                  CO__fft_coefficient__attr_\"angle\"__coeff_87  \nid                                                             \n2_trans_2001.csv                                   126.930048  \n2_trans_2998.csv                                   -51.262047  \n2_trans_437.csv                                    126.938093  \n2_trans_642.csv                                    127.082097  \n2_trans_3000.csv                                   132.973738  \n...                                                       ...  \n2_trans_41.csv                                     127.090967  \n2_trans_975.csv                                    126.944345  \n2_trans_2303.csv                                   127.106746  \n2_trans_1460.csv                                   127.064137  \n2_trans_54.csv                                     127.039351  \n\n[1575 rows x 2427 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C2H2__fft_coefficient__attr_\"real\"__coeff_51</th>\n      <th>C2H4__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.6__ql_0.4</th>\n      <th>C2H2__fft_coefficient__attr_\"real\"__coeff_22</th>\n      <th>CO__fft_coefficient__attr_\"angle\"__coeff_89</th>\n      <th>H2__fft_coefficient__attr_\"abs\"__coeff_58</th>\n      <th>H2__fft_coefficient__attr_\"abs\"__coeff_21</th>\n      <th>CO__agg_linear_trend__attr_\"rvalue\"__chunk_len_10__f_agg_\"min\"</th>\n      <th>CO__fft_coefficient__attr_\"imag\"__coeff_61</th>\n      <th>C2H4__ar_coefficient__coeff_3__k_10</th>\n      <th>C2H4__fft_coefficient__attr_\"imag\"__coeff_19</th>\n      <th>...</th>\n      <th>CO__cwt_coefficients__coeff_1__w_2__widths_(2, 5, 10, 20)</th>\n      <th>C2H4__symmetry_looking__r_0.1</th>\n      <th>C2H4__fft_coefficient__attr_\"imag\"__coeff_25</th>\n      <th>C2H4__cwt_coefficients__coeff_5__w_10__widths_(2, 5, 10, 20)</th>\n      <th>CO__cwt_coefficients__coeff_5__w_5__widths_(2, 5, 10, 20)</th>\n      <th>CO__fft_coefficient__attr_\"abs\"__coeff_51</th>\n      <th>C2H4__cwt_coefficients__coeff_1__w_5__widths_(2, 5, 10, 20)</th>\n      <th>C2H2__lempel_ziv_complexity__bins_5</th>\n      <th>CO__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.8__ql_0.6</th>\n      <th>CO__fft_coefficient__attr_\"angle\"__coeff_87</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2_trans_2001.csv</th>\n      <td>-0.000076</td>\n      <td>1.753367e-06</td>\n      <td>-0.000066</td>\n      <td>127.797766</td>\n      <td>0.000622</td>\n      <td>0.001674</td>\n      <td>0.709943</td>\n      <td>0.006630</td>\n      <td>3.618489</td>\n      <td>0.004326</td>\n      <td>...</td>\n      <td>0.001976</td>\n      <td>1.0</td>\n      <td>0.003271</td>\n      <td>0.000419</td>\n      <td>0.004151</td>\n      <td>0.008701</td>\n      <td>0.000140</td>\n      <td>0.128571</td>\n      <td>1.969936e-10</td>\n      <td>126.930048</td>\n    </tr>\n    <tr>\n      <th>2_trans_2998.csv</th>\n      <td>-0.000008</td>\n      <td>7.463647e-07</td>\n      <td>-0.000001</td>\n      <td>-50.449042</td>\n      <td>0.000739</td>\n      <td>0.001986</td>\n      <td>-0.396254</td>\n      <td>-0.000372</td>\n      <td>-0.957440</td>\n      <td>0.001282</td>\n      <td>...</td>\n      <td>0.000461</td>\n      <td>1.0</td>\n      <td>0.000972</td>\n      <td>0.000363</td>\n      <td>0.000974</td>\n      <td>0.000501</td>\n      <td>0.000116</td>\n      <td>0.150000</td>\n      <td>2.943249e-11</td>\n      <td>-51.262047</td>\n    </tr>\n    <tr>\n      <th>2_trans_437.csv</th>\n      <td>-0.000025</td>\n      <td>3.602815e-07</td>\n      <td>-0.000023</td>\n      <td>127.806625</td>\n      <td>0.000247</td>\n      <td>0.000665</td>\n      <td>0.856590</td>\n      <td>0.008115</td>\n      <td>3.484438</td>\n      <td>0.000406</td>\n      <td>...</td>\n      <td>0.001508</td>\n      <td>1.0</td>\n      <td>0.000307</td>\n      <td>0.001083</td>\n      <td>0.003186</td>\n      <td>0.010649</td>\n      <td>0.000346</td>\n      <td>0.145238</td>\n      <td>1.345016e-10</td>\n      <td>126.938093</td>\n    </tr>\n    <tr>\n      <th>2_trans_642.csv</th>\n      <td>-0.000071</td>\n      <td>2.613900e-06</td>\n      <td>-0.000065</td>\n      <td>127.947039</td>\n      <td>0.000404</td>\n      <td>0.001087</td>\n      <td>0.850631</td>\n      <td>0.021821</td>\n      <td>2.471616</td>\n      <td>0.016747</td>\n      <td>...</td>\n      <td>0.006027</td>\n      <td>0.0</td>\n      <td>0.012657</td>\n      <td>0.008711</td>\n      <td>0.012729</td>\n      <td>0.028690</td>\n      <td>0.002743</td>\n      <td>0.130952</td>\n      <td>2.248016e-10</td>\n      <td>127.082097</td>\n    </tr>\n    <tr>\n      <th>2_trans_3000.csv</th>\n      <td>-0.000012</td>\n      <td>1.689256e-06</td>\n      <td>-0.000010</td>\n      <td>127.195458</td>\n      <td>0.000878</td>\n      <td>0.002359</td>\n      <td>0.412096</td>\n      <td>0.000681</td>\n      <td>-1.444416</td>\n      <td>0.001594</td>\n      <td>...</td>\n      <td>0.000154</td>\n      <td>1.0</td>\n      <td>0.001207</td>\n      <td>0.000008</td>\n      <td>0.000337</td>\n      <td>0.001097</td>\n      <td>0.000007</td>\n      <td>0.150000</td>\n      <td>1.669509e-11</td>\n      <td>132.973738</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2_trans_41.csv</th>\n      <td>-0.000023</td>\n      <td>1.315060e-06</td>\n      <td>-0.000019</td>\n      <td>127.954656</td>\n      <td>0.002077</td>\n      <td>0.005582</td>\n      <td>0.962846</td>\n      <td>0.008649</td>\n      <td>3.510461</td>\n      <td>0.007937</td>\n      <td>...</td>\n      <td>0.015438</td>\n      <td>1.0</td>\n      <td>0.006004</td>\n      <td>0.006966</td>\n      <td>0.032685</td>\n      <td>0.011374</td>\n      <td>0.002203</td>\n      <td>0.133333</td>\n      <td>5.262857e-11</td>\n      <td>127.090967</td>\n    </tr>\n    <tr>\n      <th>2_trans_975.csv</th>\n      <td>-0.000018</td>\n      <td>3.071787e-07</td>\n      <td>-0.000014</td>\n      <td>127.811576</td>\n      <td>0.000400</td>\n      <td>0.001076</td>\n      <td>0.301402</td>\n      <td>0.004073</td>\n      <td>2.757969</td>\n      <td>0.006058</td>\n      <td>...</td>\n      <td>0.003964</td>\n      <td>1.0</td>\n      <td>0.004580</td>\n      <td>0.003229</td>\n      <td>0.008339</td>\n      <td>0.005347</td>\n      <td>0.001020</td>\n      <td>0.145238</td>\n      <td>7.059611e-11</td>\n      <td>126.944345</td>\n    </tr>\n    <tr>\n      <th>2_trans_2303.csv</th>\n      <td>-0.000022</td>\n      <td>1.486050e-06</td>\n      <td>-0.000022</td>\n      <td>127.968230</td>\n      <td>0.001346</td>\n      <td>0.003620</td>\n      <td>0.574719</td>\n      <td>0.006436</td>\n      <td>2.723769</td>\n      <td>0.007123</td>\n      <td>...</td>\n      <td>0.014063</td>\n      <td>0.0</td>\n      <td>0.005383</td>\n      <td>0.001727</td>\n      <td>0.029762</td>\n      <td>0.008466</td>\n      <td>0.000536</td>\n      <td>0.145238</td>\n      <td>2.848821e-10</td>\n      <td>127.106746</td>\n    </tr>\n    <tr>\n      <th>2_trans_1460.csv</th>\n      <td>-0.000071</td>\n      <td>1.720797e-06</td>\n      <td>-0.000060</td>\n      <td>127.929090</td>\n      <td>0.000412</td>\n      <td>0.001109</td>\n      <td>0.911751</td>\n      <td>0.005491</td>\n      <td>3.047808</td>\n      <td>0.016432</td>\n      <td>...</td>\n      <td>0.014844</td>\n      <td>0.0</td>\n      <td>0.012421</td>\n      <td>0.000039</td>\n      <td>0.031416</td>\n      <td>0.007217</td>\n      <td>0.000021</td>\n      <td>0.128571</td>\n      <td>2.426484e-11</td>\n      <td>127.064137</td>\n    </tr>\n    <tr>\n      <th>2_trans_54.csv</th>\n      <td>-0.000038</td>\n      <td>6.544150e-07</td>\n      <td>-0.000035</td>\n      <td>127.903069</td>\n      <td>0.001417</td>\n      <td>0.003807</td>\n      <td>0.966420</td>\n      <td>0.006737</td>\n      <td>3.438872</td>\n      <td>0.002730</td>\n      <td>...</td>\n      <td>0.004812</td>\n      <td>1.0</td>\n      <td>0.002065</td>\n      <td>0.009053</td>\n      <td>0.010185</td>\n      <td>0.008852</td>\n      <td>0.002856</td>\n      <td>0.147619</td>\n      <td>4.913253e-11</td>\n      <td>127.039351</td>\n    </tr>\n  </tbody>\n</table>\n<p>1575 rows × 2427 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "StSc = StandardScaler()\n",
    "X_train_sc = StSc.fit_transform(X_train_filtered)\n",
    "X_val_sc = StSc.transform(X_val_filtered)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T09:06:59.751085Z",
     "iopub.execute_input": "2023-08-20T09:06:59.751446Z",
     "iopub.status.idle": "2023-08-20T09:06:59.889472Z",
     "shell.execute_reply.started": "2023-08-20T09:06:59.751420Z",
     "shell.execute_reply": "2023-08-20T09:06:59.888301Z"
    },
    "trusted": true
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    " pd.Series(y_train)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T10:07:39.187217Z",
     "iopub.execute_input": "2023-08-20T10:07:39.187601Z",
     "iopub.status.idle": "2023-08-20T10:07:39.230969Z",
     "shell.execute_reply.started": "2023-08-20T10:07:39.187572Z",
     "shell.execute_reply": "2023-08-20T10:07:39.229209Z"
    },
    "trusted": true
   },
   "execution_count": 73,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_31/316572265.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSeries\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    382\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    383\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    384\u001B[0m             \u001B[0mname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mibase\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaybe_extract_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    385\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 386\u001B[0;31m             \u001B[0;32mif\u001B[0m \u001B[0mis_empty_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mdtype\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    387\u001B[0m                 \u001B[0;31m# gh-17261\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    388\u001B[0m                 warnings.warn(\n\u001B[1;32m    389\u001B[0m                     \u001B[0;34m\"The default dtype for empty Series will be 'object' instead \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/construction.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m    873\u001B[0m     \u001B[0mbool\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    874\u001B[0m     \"\"\"\n\u001B[1;32m    875\u001B[0m     \u001B[0mis_none\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    876\u001B[0m     \u001B[0mis_list_like_without_dtype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mis_list_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"dtype\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 877\u001B[0;31m     \u001B[0mis_simple_empty\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mis_list_like_without_dtype\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    878\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mis_none\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mis_simple_empty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1525\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mfinal\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1526\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__nonzero__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mNoReturn\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1527\u001B[0;31m         raise ValueError(\n\u001B[0m\u001B[1;32m   1528\u001B[0m             \u001B[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1529\u001B[0m             \u001B[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1530\u001B[0m         )\n",
      "\u001B[0;31mValueError\u001B[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ],
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "LogReg = LogisticRegression(solver='saga', multi_class='ovr', max_iter=200, verbose=0, n_jobs=-1)\n",
    "LogReg.fit(pd.DataFrame(X_train_sc).dropna(axis=1).values, y_train)\n",
    "\n",
    "y_train_pred_outliers = pd.DataFrame(LogReg.predict(X_train_sc), \n",
    "                                     index=y_train.index, \n",
    "                                     columns=y_train.columns)\n",
    "y_val_pred_outliers = pd.DataFrame(LogReg.predict(X_val_sc), \n",
    "                                   index=y_val.index, \n",
    "                                   columns=y_val.columns)\n",
    "\n",
    "f1_train = round(f1_score(y_train, y_train_pred_outliers), 2)\n",
    "f1_val = round(f1_score(y_val, y_val_pred_outliers), 2)\n",
    "print(f'F1 on train set = {f1_train}')\n",
    "print(f'F1 on test set = {f1_val}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T10:16:00.236657Z",
     "iopub.execute_input": "2023-08-20T10:16:00.237041Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stage 2: linear regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "y = y_data.copy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "y_train_wo_outliers = y_train[y_train != 1093].dropna()\n",
    "X_train_wo_outliers = X_train.loc[y_train_wo_outliers.index]\n",
    "\n",
    "StSc = StandardScaler()\n",
    "X_train_wo_outliers_sc = StSc.fit_transform(X_train_wo_outliers)\n",
    "X_train_sc = StSc.transform(X_train)\n",
    "X_val_sc = StSc.transform(X_val)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-24T15:48:51.743992Z",
     "iopub.execute_input": "2023-06-24T15:48:51.744658Z",
     "iopub.status.idle": "2023-06-24T15:48:51.786578Z",
     "shell.execute_reply.started": "2023-06-24T15:48:51.744618Z",
     "shell.execute_reply": "2023-06-24T15:48:51.785066Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_wo_outliers_sc, y_train_wo_outliers)\n",
    "\n",
    "y_train_pred = pd.DataFrame(lr.predict(X_train_sc), \n",
    "                            index=y_train.index, \n",
    "                            columns=y_train.columns)\n",
    "y_val_pred = pd.DataFrame(lr.predict(X_val_sc), \n",
    "                          index=y_val.index, \n",
    "                          columns=y_val.columns)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-24T15:48:51.788529Z",
     "iopub.execute_input": "2023-06-24T15:48:51.789324Z",
     "iopub.status.idle": "2023-06-24T15:48:51.810783Z",
     "shell.execute_reply.started": "2023-06-24T15:48:51.789286Z",
     "shell.execute_reply": "2023-06-24T15:48:51.809284Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ind = y_train_pred_outliers[y_train_pred_outliers['predicted']==1].index\n",
    "y_train_pred.loc[ind] = 1093\n",
    "\n",
    "ind = y_val_pred_outliers[y_val_pred_outliers['predicted']==1].index\n",
    "y_val_pred.loc[ind] = 1093\n",
    "\n",
    "mae_train = round(mean_absolute_error(y_train, y_train_pred), 2)\n",
    "mae_val = round(mean_absolute_error(y_val, y_val_pred), 2)\n",
    "print(f'MAE on train set = {mae_train}')\n",
    "print(f'MAE on test set = {mae_val}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-24T15:48:51.81288Z",
     "iopub.execute_input": "2023-06-24T15:48:51.813667Z",
     "iopub.status.idle": "2023-06-24T15:48:51.860747Z",
     "shell.execute_reply.started": "2023-06-24T15:48:51.81363Z",
     "shell.execute_reply": "2023-06-24T15:48:51.85943Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is better than previous linear regression. Previous metrics:\n",
    "- MAE on train set = 140.82\n",
    "- MAE on test set = 148.21"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stage 2: gradient boosting regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "y = y_data.copy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "y_train_wo_outliers = y_train[y_train != 1093].dropna()\n",
    "X_train_wo_outliers = X_train.loc[y_train_wo_outliers.index]\n",
    "\n",
    "StSc = StandardScaler()\n",
    "X_train_wo_outliers_sc = StSc.fit_transform(X_train_wo_outliers)\n",
    "X_train_sc = StSc.transform(X_train)\n",
    "X_val_sc = StSc.transform(X_val)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T09:09:17.532086Z",
     "iopub.execute_input": "2023-08-20T09:09:17.532441Z",
     "iopub.status.idle": "2023-08-20T09:09:17.755291Z",
     "shell.execute_reply.started": "2023-08-20T09:09:17.532418Z",
     "shell.execute_reply": "2023-08-20T09:09:17.753790Z"
    },
    "trusted": true
   },
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/sklearn/utils/extmath.py:1047: RuntimeWarning: invalid value encountered in divide\n  updated_mean = (last_sum + new_sum) / updated_sample_count\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/extmath.py:1052: RuntimeWarning: invalid value encountered in divide\n  T = new_sum / new_sample_count\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/extmath.py:1072: RuntimeWarning: invalid value encountered in divide\n  new_unnormalized_variance -= correction**2 / new_sample_count\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cbr = CatBoostRegressor(random_state=1, verbose=0)\n",
    "cbr.fit(X_train_wo_outliers_sc, y_train_wo_outliers)\n",
    "\n",
    "y_train_pred = pd.DataFrame(cbr.predict(X_train_sc), \n",
    "                            index=y_train.index, \n",
    "                            columns=y_train.columns)\n",
    "y_val_pred = pd.DataFrame(cbr.predict(X_val_sc), \n",
    "                          index=y_val.index, \n",
    "                          columns=y_val.columns)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T09:11:30.966026Z",
     "iopub.execute_input": "2023-08-20T09:11:30.966393Z",
     "iopub.status.idle": "2023-08-20T09:15:33.835730Z",
     "shell.execute_reply.started": "2023-08-20T09:11:30.966367Z",
     "shell.execute_reply": "2023-08-20T09:15:33.834780Z"
    },
    "trusted": true
   },
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "text": "Learning rate set to 0.041185\n0:\tlearn: 139.8466808\ttotal: 293ms\tremaining: 4m 52s\n1:\tlearn: 137.1775501\ttotal: 526ms\tremaining: 4m 22s\n2:\tlearn: 134.9267130\ttotal: 771ms\tremaining: 4m 16s\n3:\tlearn: 132.5427424\ttotal: 1s\tremaining: 4m 9s\n4:\tlearn: 130.5025750\ttotal: 1.28s\tremaining: 4m 14s\n5:\tlearn: 128.2323709\ttotal: 1.56s\tremaining: 4m 18s\n6:\tlearn: 126.1533334\ttotal: 1.79s\tremaining: 4m 14s\n7:\tlearn: 123.9418519\ttotal: 2.03s\tremaining: 4m 11s\n8:\tlearn: 121.8900428\ttotal: 2.27s\tremaining: 4m 9s\n9:\tlearn: 119.9955631\ttotal: 2.5s\tremaining: 4m 7s\n10:\tlearn: 118.3009064\ttotal: 2.74s\tremaining: 4m 6s\n11:\tlearn: 116.4573767\ttotal: 2.97s\tremaining: 4m 4s\n12:\tlearn: 114.7816295\ttotal: 3.2s\tremaining: 4m 2s\n13:\tlearn: 113.1619097\ttotal: 3.42s\tremaining: 4m 1s\n14:\tlearn: 111.5908115\ttotal: 3.65s\tremaining: 4m\n15:\tlearn: 110.2375513\ttotal: 3.9s\tremaining: 3m 59s\n16:\tlearn: 108.9279376\ttotal: 4.14s\tremaining: 3m 59s\n17:\tlearn: 107.4568944\ttotal: 4.37s\tremaining: 3m 58s\n18:\tlearn: 106.2309128\ttotal: 4.61s\tremaining: 3m 57s\n19:\tlearn: 104.9995560\ttotal: 4.84s\tremaining: 3m 57s\n20:\tlearn: 103.7119953\ttotal: 5.08s\tremaining: 3m 57s\n21:\tlearn: 102.5583920\ttotal: 5.33s\tremaining: 3m 57s\n22:\tlearn: 101.3844954\ttotal: 5.57s\tremaining: 3m 56s\n23:\tlearn: 100.2797614\ttotal: 5.82s\tremaining: 3m 56s\n24:\tlearn: 99.2545490\ttotal: 6.06s\tremaining: 3m 56s\n25:\tlearn: 98.2461776\ttotal: 6.29s\tremaining: 3m 55s\n26:\tlearn: 97.5855493\ttotal: 6.53s\tremaining: 3m 55s\n27:\tlearn: 96.7095367\ttotal: 6.76s\tremaining: 3m 54s\n28:\tlearn: 96.2050656\ttotal: 7s\tremaining: 3m 54s\n29:\tlearn: 95.4943629\ttotal: 7.24s\tremaining: 3m 54s\n30:\tlearn: 94.6897560\ttotal: 7.48s\tremaining: 3m 53s\n31:\tlearn: 93.9192577\ttotal: 7.71s\tremaining: 3m 53s\n32:\tlearn: 93.1781715\ttotal: 7.95s\tremaining: 3m 52s\n33:\tlearn: 92.5602187\ttotal: 8.19s\tremaining: 3m 52s\n34:\tlearn: 91.8684395\ttotal: 8.43s\tremaining: 3m 52s\n35:\tlearn: 91.1124757\ttotal: 8.67s\tremaining: 3m 52s\n36:\tlearn: 90.6695581\ttotal: 8.91s\tremaining: 3m 51s\n37:\tlearn: 90.2168422\ttotal: 9.16s\tremaining: 3m 51s\n38:\tlearn: 89.4000352\ttotal: 9.4s\tremaining: 3m 51s\n39:\tlearn: 88.7355881\ttotal: 9.64s\tremaining: 3m 51s\n40:\tlearn: 88.2137072\ttotal: 9.88s\tremaining: 3m 50s\n41:\tlearn: 87.6045132\ttotal: 10.1s\tremaining: 3m 50s\n42:\tlearn: 87.2698037\ttotal: 10.4s\tremaining: 3m 50s\n43:\tlearn: 86.8574519\ttotal: 10.6s\tremaining: 3m 49s\n44:\tlearn: 86.3028703\ttotal: 10.8s\tremaining: 3m 49s\n45:\tlearn: 85.8952533\ttotal: 11.1s\tremaining: 3m 49s\n46:\tlearn: 85.3909349\ttotal: 11.3s\tremaining: 3m 48s\n47:\tlearn: 85.1205586\ttotal: 11.5s\tremaining: 3m 48s\n48:\tlearn: 84.8833349\ttotal: 11.8s\tremaining: 3m 48s\n49:\tlearn: 84.5536374\ttotal: 12s\tremaining: 3m 48s\n50:\tlearn: 84.3359484\ttotal: 12.2s\tremaining: 3m 47s\n51:\tlearn: 83.8101720\ttotal: 12.5s\tremaining: 3m 47s\n52:\tlearn: 83.4651829\ttotal: 12.7s\tremaining: 3m 47s\n53:\tlearn: 82.9777984\ttotal: 13s\tremaining: 3m 47s\n54:\tlearn: 82.6343241\ttotal: 13.2s\tremaining: 3m 46s\n55:\tlearn: 82.2515774\ttotal: 13.4s\tremaining: 3m 46s\n56:\tlearn: 81.8552893\ttotal: 13.7s\tremaining: 3m 46s\n57:\tlearn: 81.2361343\ttotal: 13.9s\tremaining: 3m 46s\n58:\tlearn: 80.9027832\ttotal: 14.2s\tremaining: 3m 45s\n59:\tlearn: 80.5566162\ttotal: 14.4s\tremaining: 3m 45s\n60:\tlearn: 80.1430413\ttotal: 14.6s\tremaining: 3m 45s\n61:\tlearn: 79.8912576\ttotal: 14.9s\tremaining: 3m 45s\n62:\tlearn: 79.5232380\ttotal: 15.2s\tremaining: 3m 45s\n63:\tlearn: 79.3069844\ttotal: 15.4s\tremaining: 3m 45s\n64:\tlearn: 79.0143485\ttotal: 15.6s\tremaining: 3m 45s\n65:\tlearn: 78.7754982\ttotal: 15.9s\tremaining: 3m 44s\n66:\tlearn: 78.3782428\ttotal: 16.1s\tremaining: 3m 44s\n67:\tlearn: 77.9155414\ttotal: 16.4s\tremaining: 3m 45s\n68:\tlearn: 77.6368520\ttotal: 16.8s\tremaining: 3m 46s\n69:\tlearn: 77.2854099\ttotal: 17.1s\tremaining: 3m 46s\n70:\tlearn: 77.0909464\ttotal: 17.3s\tremaining: 3m 46s\n71:\tlearn: 76.7721522\ttotal: 17.6s\tremaining: 3m 46s\n72:\tlearn: 76.4710680\ttotal: 17.8s\tremaining: 3m 46s\n73:\tlearn: 76.0800403\ttotal: 18.1s\tremaining: 3m 45s\n74:\tlearn: 75.8087574\ttotal: 18.3s\tremaining: 3m 45s\n75:\tlearn: 75.5382377\ttotal: 18.5s\tremaining: 3m 45s\n76:\tlearn: 75.2789266\ttotal: 18.8s\tremaining: 3m 44s\n77:\tlearn: 75.0781298\ttotal: 19s\tremaining: 3m 44s\n78:\tlearn: 74.7492741\ttotal: 19.2s\tremaining: 3m 44s\n79:\tlearn: 74.4922535\ttotal: 19.5s\tremaining: 3m 44s\n80:\tlearn: 74.2255181\ttotal: 19.7s\tremaining: 3m 43s\n81:\tlearn: 73.8696517\ttotal: 20s\tremaining: 3m 43s\n82:\tlearn: 73.6833525\ttotal: 20.2s\tremaining: 3m 43s\n83:\tlearn: 73.3913686\ttotal: 20.4s\tremaining: 3m 42s\n84:\tlearn: 73.2271149\ttotal: 20.7s\tremaining: 3m 42s\n85:\tlearn: 73.0761306\ttotal: 20.9s\tremaining: 3m 42s\n86:\tlearn: 72.8714154\ttotal: 21.1s\tremaining: 3m 41s\n87:\tlearn: 72.5006505\ttotal: 21.4s\tremaining: 3m 41s\n88:\tlearn: 72.2287020\ttotal: 21.6s\tremaining: 3m 41s\n89:\tlearn: 71.9992508\ttotal: 21.8s\tremaining: 3m 40s\n90:\tlearn: 71.8155204\ttotal: 22.1s\tremaining: 3m 40s\n91:\tlearn: 71.5449964\ttotal: 22.3s\tremaining: 3m 40s\n92:\tlearn: 71.3178556\ttotal: 22.6s\tremaining: 3m 39s\n93:\tlearn: 71.1670030\ttotal: 22.8s\tremaining: 3m 39s\n94:\tlearn: 70.8630126\ttotal: 23s\tremaining: 3m 39s\n95:\tlearn: 70.6897228\ttotal: 23.3s\tremaining: 3m 39s\n96:\tlearn: 70.4761365\ttotal: 23.5s\tremaining: 3m 38s\n97:\tlearn: 70.2777198\ttotal: 23.7s\tremaining: 3m 38s\n98:\tlearn: 70.1403741\ttotal: 24s\tremaining: 3m 38s\n99:\tlearn: 69.8638717\ttotal: 24.2s\tremaining: 3m 38s\n100:\tlearn: 69.6507471\ttotal: 24.5s\tremaining: 3m 37s\n101:\tlearn: 69.4334290\ttotal: 24.7s\tremaining: 3m 37s\n102:\tlearn: 69.2391015\ttotal: 25s\tremaining: 3m 37s\n103:\tlearn: 69.0643722\ttotal: 25.2s\tremaining: 3m 37s\n104:\tlearn: 68.8352641\ttotal: 25.5s\tremaining: 3m 37s\n105:\tlearn: 68.7282593\ttotal: 25.7s\tremaining: 3m 36s\n106:\tlearn: 68.6054932\ttotal: 25.9s\tremaining: 3m 36s\n107:\tlearn: 68.4540864\ttotal: 26.2s\tremaining: 3m 36s\n108:\tlearn: 68.2853520\ttotal: 26.4s\tremaining: 3m 35s\n109:\tlearn: 68.1271789\ttotal: 26.6s\tremaining: 3m 35s\n110:\tlearn: 67.9264429\ttotal: 26.9s\tremaining: 3m 35s\n111:\tlearn: 67.7118974\ttotal: 27.1s\tremaining: 3m 35s\n112:\tlearn: 67.4490128\ttotal: 27.4s\tremaining: 3m 34s\n113:\tlearn: 67.3202859\ttotal: 27.6s\tremaining: 3m 34s\n114:\tlearn: 67.1444526\ttotal: 27.8s\tremaining: 3m 34s\n115:\tlearn: 66.9599953\ttotal: 28.1s\tremaining: 3m 33s\n116:\tlearn: 66.7902501\ttotal: 28.3s\tremaining: 3m 33s\n117:\tlearn: 66.6115549\ttotal: 28.6s\tremaining: 3m 33s\n118:\tlearn: 66.3896568\ttotal: 28.8s\tremaining: 3m 33s\n119:\tlearn: 66.1573075\ttotal: 29s\tremaining: 3m 32s\n120:\tlearn: 66.0017000\ttotal: 29.3s\tremaining: 3m 32s\n121:\tlearn: 65.7958938\ttotal: 29.5s\tremaining: 3m 32s\n122:\tlearn: 65.6883172\ttotal: 29.7s\tremaining: 3m 32s\n123:\tlearn: 65.5464070\ttotal: 30s\tremaining: 3m 31s\n124:\tlearn: 65.4268572\ttotal: 30.2s\tremaining: 3m 31s\n125:\tlearn: 65.2533760\ttotal: 30.5s\tremaining: 3m 31s\n126:\tlearn: 65.1108781\ttotal: 30.7s\tremaining: 3m 31s\n127:\tlearn: 64.9498200\ttotal: 30.9s\tremaining: 3m 30s\n128:\tlearn: 64.6709694\ttotal: 31.2s\tremaining: 3m 30s\n129:\tlearn: 64.5483324\ttotal: 31.4s\tremaining: 3m 30s\n130:\tlearn: 64.3518219\ttotal: 31.7s\tremaining: 3m 30s\n131:\tlearn: 64.1452828\ttotal: 31.9s\tremaining: 3m 29s\n132:\tlearn: 63.9363900\ttotal: 32.2s\tremaining: 3m 29s\n133:\tlearn: 63.7128799\ttotal: 32.4s\tremaining: 3m 29s\n134:\tlearn: 63.4980982\ttotal: 32.6s\tremaining: 3m 29s\n135:\tlearn: 63.3285456\ttotal: 32.9s\tremaining: 3m 28s\n136:\tlearn: 63.1095568\ttotal: 33.1s\tremaining: 3m 28s\n137:\tlearn: 62.9162625\ttotal: 33.4s\tremaining: 3m 28s\n138:\tlearn: 62.7971554\ttotal: 33.6s\tremaining: 3m 28s\n139:\tlearn: 62.6642350\ttotal: 33.8s\tremaining: 3m 27s\n140:\tlearn: 62.5715529\ttotal: 34.1s\tremaining: 3m 27s\n141:\tlearn: 62.4080371\ttotal: 34.3s\tremaining: 3m 27s\n142:\tlearn: 62.2194252\ttotal: 34.5s\tremaining: 3m 27s\n143:\tlearn: 62.0513747\ttotal: 34.8s\tremaining: 3m 26s\n144:\tlearn: 61.8771086\ttotal: 35s\tremaining: 3m 26s\n145:\tlearn: 61.7040504\ttotal: 35.3s\tremaining: 3m 26s\n146:\tlearn: 61.5554448\ttotal: 35.5s\tremaining: 3m 26s\n147:\tlearn: 61.4404634\ttotal: 35.7s\tremaining: 3m 25s\n148:\tlearn: 61.1424248\ttotal: 36s\tremaining: 3m 25s\n149:\tlearn: 61.0643293\ttotal: 36.2s\tremaining: 3m 25s\n150:\tlearn: 60.8474388\ttotal: 36.5s\tremaining: 3m 25s\n151:\tlearn: 60.6360672\ttotal: 36.7s\tremaining: 3m 24s\n152:\tlearn: 60.3551272\ttotal: 36.9s\tremaining: 3m 24s\n153:\tlearn: 60.2209660\ttotal: 37.2s\tremaining: 3m 24s\n154:\tlearn: 60.1665638\ttotal: 37.4s\tremaining: 3m 23s\n155:\tlearn: 60.0267095\ttotal: 37.6s\tremaining: 3m 23s\n156:\tlearn: 59.8348263\ttotal: 37.9s\tremaining: 3m 23s\n157:\tlearn: 59.6691881\ttotal: 38.1s\tremaining: 3m 23s\n158:\tlearn: 59.5882465\ttotal: 38.3s\tremaining: 3m 22s\n159:\tlearn: 59.5333402\ttotal: 38.6s\tremaining: 3m 22s\n160:\tlearn: 59.4286924\ttotal: 38.8s\tremaining: 3m 22s\n161:\tlearn: 59.2482082\ttotal: 39.1s\tremaining: 3m 22s\n162:\tlearn: 59.0313290\ttotal: 39.3s\tremaining: 3m 21s\n163:\tlearn: 58.8413967\ttotal: 39.5s\tremaining: 3m 21s\n164:\tlearn: 58.7328470\ttotal: 39.8s\tremaining: 3m 21s\n165:\tlearn: 58.5628089\ttotal: 40s\tremaining: 3m 21s\n166:\tlearn: 58.4547807\ttotal: 40.3s\tremaining: 3m 20s\n167:\tlearn: 58.2948453\ttotal: 40.5s\tremaining: 3m 20s\n168:\tlearn: 58.1434303\ttotal: 40.7s\tremaining: 3m 20s\n169:\tlearn: 57.9707961\ttotal: 41s\tremaining: 3m 20s\n170:\tlearn: 57.8240749\ttotal: 41.2s\tremaining: 3m 19s\n171:\tlearn: 57.7488595\ttotal: 41.4s\tremaining: 3m 19s\n172:\tlearn: 57.6307674\ttotal: 41.7s\tremaining: 3m 19s\n173:\tlearn: 57.4619952\ttotal: 41.9s\tremaining: 3m 18s\n174:\tlearn: 57.2803574\ttotal: 42.1s\tremaining: 3m 18s\n175:\tlearn: 57.1857649\ttotal: 42.4s\tremaining: 3m 18s\n176:\tlearn: 57.0960048\ttotal: 42.6s\tremaining: 3m 18s\n177:\tlearn: 56.9062769\ttotal: 42.9s\tremaining: 3m 17s\n178:\tlearn: 56.7664075\ttotal: 43.1s\tremaining: 3m 17s\n179:\tlearn: 56.6656163\ttotal: 43.3s\tremaining: 3m 17s\n180:\tlearn: 56.5179083\ttotal: 43.6s\tremaining: 3m 17s\n181:\tlearn: 56.3199677\ttotal: 43.8s\tremaining: 3m 16s\n182:\tlearn: 56.1164377\ttotal: 44s\tremaining: 3m 16s\n183:\tlearn: 55.8706918\ttotal: 44.3s\tremaining: 3m 16s\n184:\tlearn: 55.6301583\ttotal: 44.5s\tremaining: 3m 16s\n185:\tlearn: 55.4559477\ttotal: 44.8s\tremaining: 3m 15s\n186:\tlearn: 55.2888907\ttotal: 45s\tremaining: 3m 15s\n187:\tlearn: 55.0764369\ttotal: 45.2s\tremaining: 3m 15s\n188:\tlearn: 54.9043569\ttotal: 45.5s\tremaining: 3m 15s\n189:\tlearn: 54.7389810\ttotal: 45.7s\tremaining: 3m 14s\n190:\tlearn: 54.6325149\ttotal: 46s\tremaining: 3m 14s\n191:\tlearn: 54.5410002\ttotal: 46.2s\tremaining: 3m 14s\n192:\tlearn: 54.3512693\ttotal: 46.4s\tremaining: 3m 14s\n193:\tlearn: 54.2578790\ttotal: 46.7s\tremaining: 3m 13s\n194:\tlearn: 54.1367074\ttotal: 46.9s\tremaining: 3m 13s\n195:\tlearn: 53.9461859\ttotal: 47.2s\tremaining: 3m 13s\n196:\tlearn: 53.8102603\ttotal: 47.4s\tremaining: 3m 13s\n197:\tlearn: 53.6566309\ttotal: 47.7s\tremaining: 3m 13s\n198:\tlearn: 53.5659809\ttotal: 48s\tremaining: 3m 13s\n199:\tlearn: 53.4324759\ttotal: 48.3s\tremaining: 3m 13s\n200:\tlearn: 53.2302346\ttotal: 48.6s\tremaining: 3m 13s\n201:\tlearn: 53.0688708\ttotal: 48.9s\tremaining: 3m 13s\n202:\tlearn: 53.0046397\ttotal: 49.1s\tremaining: 3m 12s\n203:\tlearn: 52.7725846\ttotal: 49.3s\tremaining: 3m 12s\n204:\tlearn: 52.5563744\ttotal: 49.6s\tremaining: 3m 12s\n205:\tlearn: 52.4114469\ttotal: 49.8s\tremaining: 3m 11s\n206:\tlearn: 52.2616217\ttotal: 50s\tremaining: 3m 11s\n207:\tlearn: 52.2073195\ttotal: 50.3s\tremaining: 3m 11s\n208:\tlearn: 52.0765814\ttotal: 50.5s\tremaining: 3m 11s\n209:\tlearn: 52.0312796\ttotal: 50.8s\tremaining: 3m 10s\n210:\tlearn: 51.8523061\ttotal: 51s\tremaining: 3m 10s\n211:\tlearn: 51.7497600\ttotal: 51.2s\tremaining: 3m 10s\n212:\tlearn: 51.5903954\ttotal: 51.5s\tremaining: 3m 10s\n213:\tlearn: 51.4290512\ttotal: 51.7s\tremaining: 3m 9s\n214:\tlearn: 51.1970544\ttotal: 52s\tremaining: 3m 9s\n215:\tlearn: 51.0064754\ttotal: 52.2s\tremaining: 3m 9s\n216:\tlearn: 50.8821756\ttotal: 52.4s\tremaining: 3m 9s\n217:\tlearn: 50.8471823\ttotal: 52.7s\tremaining: 3m 8s\n218:\tlearn: 50.6937079\ttotal: 52.9s\tremaining: 3m 8s\n219:\tlearn: 50.5447299\ttotal: 53.2s\tremaining: 3m 8s\n220:\tlearn: 50.3666695\ttotal: 53.4s\tremaining: 3m 8s\n221:\tlearn: 50.3126201\ttotal: 53.6s\tremaining: 3m 8s\n222:\tlearn: 50.0932127\ttotal: 53.9s\tremaining: 3m 7s\n223:\tlearn: 49.9327953\ttotal: 54.1s\tremaining: 3m 7s\n224:\tlearn: 49.8376941\ttotal: 54.4s\tremaining: 3m 7s\n225:\tlearn: 49.6188866\ttotal: 54.6s\tremaining: 3m 7s\n226:\tlearn: 49.4838765\ttotal: 54.9s\tremaining: 3m 6s\n227:\tlearn: 49.3309488\ttotal: 55.1s\tremaining: 3m 6s\n228:\tlearn: 49.2185831\ttotal: 55.4s\tremaining: 3m 6s\n229:\tlearn: 49.0866167\ttotal: 55.6s\tremaining: 3m 6s\n230:\tlearn: 48.8840473\ttotal: 55.8s\tremaining: 3m 5s\n231:\tlearn: 48.6410710\ttotal: 56.1s\tremaining: 3m 5s\n232:\tlearn: 48.4833174\ttotal: 56.3s\tremaining: 3m 5s\n233:\tlearn: 48.3185580\ttotal: 56.5s\tremaining: 3m 5s\n234:\tlearn: 48.2353337\ttotal: 56.8s\tremaining: 3m 4s\n235:\tlearn: 48.0975007\ttotal: 57s\tremaining: 3m 4s\n236:\tlearn: 47.9552976\ttotal: 57.3s\tremaining: 3m 4s\n237:\tlearn: 47.8384336\ttotal: 57.5s\tremaining: 3m 4s\n238:\tlearn: 47.7187173\ttotal: 57.7s\tremaining: 3m 3s\n239:\tlearn: 47.6154960\ttotal: 58s\tremaining: 3m 3s\n240:\tlearn: 47.4904184\ttotal: 58.2s\tremaining: 3m 3s\n241:\tlearn: 47.3468291\ttotal: 58.5s\tremaining: 3m 3s\n242:\tlearn: 47.2991673\ttotal: 58.7s\tremaining: 3m 2s\n243:\tlearn: 47.0868387\ttotal: 58.9s\tremaining: 3m 2s\n244:\tlearn: 47.0427248\ttotal: 59.2s\tremaining: 3m 2s\n245:\tlearn: 46.9277061\ttotal: 59.4s\tremaining: 3m 2s\n246:\tlearn: 46.8345565\ttotal: 59.6s\tremaining: 3m 1s\n247:\tlearn: 46.7907588\ttotal: 59.9s\tremaining: 3m 1s\n248:\tlearn: 46.6461926\ttotal: 1m\tremaining: 3m 1s\n249:\tlearn: 46.5023172\ttotal: 1m\tremaining: 3m 1s\n250:\tlearn: 46.3160986\ttotal: 1m\tremaining: 3m\n251:\tlearn: 46.0790606\ttotal: 1m\tremaining: 3m\n252:\tlearn: 45.9360722\ttotal: 1m 1s\tremaining: 3m\n253:\tlearn: 45.7735435\ttotal: 1m 1s\tremaining: 3m\n254:\tlearn: 45.6194702\ttotal: 1m 1s\tremaining: 2m 59s\n255:\tlearn: 45.4619262\ttotal: 1m 1s\tremaining: 2m 59s\n256:\tlearn: 45.4132330\ttotal: 1m 2s\tremaining: 2m 59s\n257:\tlearn: 45.2952504\ttotal: 1m 2s\tremaining: 2m 59s\n258:\tlearn: 45.2514881\ttotal: 1m 2s\tremaining: 2m 58s\n259:\tlearn: 45.0230931\ttotal: 1m 2s\tremaining: 2m 58s\n260:\tlearn: 44.9388444\ttotal: 1m 3s\tremaining: 2m 58s\n261:\tlearn: 44.9005251\ttotal: 1m 3s\tremaining: 2m 58s\n262:\tlearn: 44.7294509\ttotal: 1m 3s\tremaining: 2m 57s\n263:\tlearn: 44.4896554\ttotal: 1m 3s\tremaining: 2m 57s\n264:\tlearn: 44.3116486\ttotal: 1m 3s\tremaining: 2m 57s\n265:\tlearn: 44.1556647\ttotal: 1m 4s\tremaining: 2m 57s\n266:\tlearn: 44.0199626\ttotal: 1m 4s\tremaining: 2m 56s\n267:\tlearn: 43.8348147\ttotal: 1m 4s\tremaining: 2m 56s\n268:\tlearn: 43.6870790\ttotal: 1m 4s\tremaining: 2m 56s\n269:\tlearn: 43.5275793\ttotal: 1m 5s\tremaining: 2m 56s\n270:\tlearn: 43.3863687\ttotal: 1m 5s\tremaining: 2m 56s\n271:\tlearn: 43.3297466\ttotal: 1m 5s\tremaining: 2m 55s\n272:\tlearn: 43.2019333\ttotal: 1m 5s\tremaining: 2m 55s\n273:\tlearn: 43.1627996\ttotal: 1m 6s\tremaining: 2m 55s\n274:\tlearn: 43.0152250\ttotal: 1m 6s\tremaining: 2m 55s\n275:\tlearn: 42.9172437\ttotal: 1m 6s\tremaining: 2m 54s\n276:\tlearn: 42.7771178\ttotal: 1m 6s\tremaining: 2m 54s\n277:\tlearn: 42.6111833\ttotal: 1m 7s\tremaining: 2m 54s\n278:\tlearn: 42.4865859\ttotal: 1m 7s\tremaining: 2m 54s\n279:\tlearn: 42.3788809\ttotal: 1m 7s\tremaining: 2m 53s\n280:\tlearn: 42.2462038\ttotal: 1m 7s\tremaining: 2m 53s\n281:\tlearn: 42.1299332\ttotal: 1m 8s\tremaining: 2m 53s\n282:\tlearn: 42.0038291\ttotal: 1m 8s\tremaining: 2m 53s\n283:\tlearn: 41.8923625\ttotal: 1m 8s\tremaining: 2m 52s\n284:\tlearn: 41.7923862\ttotal: 1m 8s\tremaining: 2m 52s\n285:\tlearn: 41.6933259\ttotal: 1m 9s\tremaining: 2m 52s\n286:\tlearn: 41.5749907\ttotal: 1m 9s\tremaining: 2m 52s\n287:\tlearn: 41.4624570\ttotal: 1m 9s\tremaining: 2m 51s\n288:\tlearn: 41.3240417\ttotal: 1m 9s\tremaining: 2m 51s\n289:\tlearn: 41.2851746\ttotal: 1m 9s\tremaining: 2m 51s\n290:\tlearn: 41.1642190\ttotal: 1m 10s\tremaining: 2m 51s\n291:\tlearn: 41.1116341\ttotal: 1m 10s\tremaining: 2m 50s\n292:\tlearn: 40.9924177\ttotal: 1m 10s\tremaining: 2m 50s\n293:\tlearn: 40.9271488\ttotal: 1m 10s\tremaining: 2m 50s\n294:\tlearn: 40.7484201\ttotal: 1m 11s\tremaining: 2m 50s\n295:\tlearn: 40.6428482\ttotal: 1m 11s\tremaining: 2m 49s\n296:\tlearn: 40.5477590\ttotal: 1m 11s\tremaining: 2m 49s\n297:\tlearn: 40.4115135\ttotal: 1m 11s\tremaining: 2m 49s\n298:\tlearn: 40.2514603\ttotal: 1m 12s\tremaining: 2m 49s\n299:\tlearn: 40.1209855\ttotal: 1m 12s\tremaining: 2m 48s\n300:\tlearn: 40.0151335\ttotal: 1m 12s\tremaining: 2m 48s\n301:\tlearn: 39.9301276\ttotal: 1m 12s\tremaining: 2m 48s\n302:\tlearn: 39.8914880\ttotal: 1m 13s\tremaining: 2m 48s\n303:\tlearn: 39.7890272\ttotal: 1m 13s\tremaining: 2m 47s\n304:\tlearn: 39.7257713\ttotal: 1m 13s\tremaining: 2m 47s\n305:\tlearn: 39.6375071\ttotal: 1m 13s\tremaining: 2m 47s\n306:\tlearn: 39.4985429\ttotal: 1m 14s\tremaining: 2m 47s\n307:\tlearn: 39.3852944\ttotal: 1m 14s\tremaining: 2m 46s\n308:\tlearn: 39.3561248\ttotal: 1m 14s\tremaining: 2m 46s\n309:\tlearn: 39.2506830\ttotal: 1m 14s\tremaining: 2m 46s\n310:\tlearn: 39.1469319\ttotal: 1m 15s\tremaining: 2m 46s\n311:\tlearn: 39.0366328\ttotal: 1m 15s\tremaining: 2m 46s\n312:\tlearn: 38.8984265\ttotal: 1m 15s\tremaining: 2m 45s\n313:\tlearn: 38.7633268\ttotal: 1m 15s\tremaining: 2m 45s\n314:\tlearn: 38.7352069\ttotal: 1m 16s\tremaining: 2m 45s\n315:\tlearn: 38.6613888\ttotal: 1m 16s\tremaining: 2m 45s\n316:\tlearn: 38.5529473\ttotal: 1m 16s\tremaining: 2m 44s\n317:\tlearn: 38.5192930\ttotal: 1m 16s\tremaining: 2m 44s\n318:\tlearn: 38.4168750\ttotal: 1m 17s\tremaining: 2m 44s\n319:\tlearn: 38.3820137\ttotal: 1m 17s\tremaining: 2m 44s\n320:\tlearn: 38.3147578\ttotal: 1m 17s\tremaining: 2m 43s\n321:\tlearn: 38.1828787\ttotal: 1m 17s\tremaining: 2m 43s\n322:\tlearn: 38.0901872\ttotal: 1m 17s\tremaining: 2m 43s\n323:\tlearn: 38.0188575\ttotal: 1m 18s\tremaining: 2m 43s\n324:\tlearn: 37.8982594\ttotal: 1m 18s\tremaining: 2m 42s\n325:\tlearn: 37.7948994\ttotal: 1m 18s\tremaining: 2m 42s\n326:\tlearn: 37.7742137\ttotal: 1m 19s\tremaining: 2m 42s\n327:\tlearn: 37.7160258\ttotal: 1m 19s\tremaining: 2m 42s\n328:\tlearn: 37.6483793\ttotal: 1m 19s\tremaining: 2m 42s\n329:\tlearn: 37.5601497\ttotal: 1m 19s\tremaining: 2m 42s\n330:\tlearn: 37.4002337\ttotal: 1m 20s\tremaining: 2m 41s\n331:\tlearn: 37.3007896\ttotal: 1m 20s\tremaining: 2m 41s\n332:\tlearn: 37.2795785\ttotal: 1m 20s\tremaining: 2m 41s\n333:\tlearn: 37.1424987\ttotal: 1m 20s\tremaining: 2m 41s\n334:\tlearn: 37.0605510\ttotal: 1m 21s\tremaining: 2m 40s\n335:\tlearn: 36.9341220\ttotal: 1m 21s\tremaining: 2m 40s\n336:\tlearn: 36.8087168\ttotal: 1m 21s\tremaining: 2m 40s\n337:\tlearn: 36.7524515\ttotal: 1m 21s\tremaining: 2m 40s\n338:\tlearn: 36.6833119\ttotal: 1m 22s\tremaining: 2m 39s\n339:\tlearn: 36.6301022\ttotal: 1m 22s\tremaining: 2m 39s\n340:\tlearn: 36.5098223\ttotal: 1m 22s\tremaining: 2m 39s\n341:\tlearn: 36.3956530\ttotal: 1m 22s\tremaining: 2m 39s\n342:\tlearn: 36.2539039\ttotal: 1m 22s\tremaining: 2m 38s\n343:\tlearn: 36.1415802\ttotal: 1m 23s\tremaining: 2m 38s\n344:\tlearn: 36.0470693\ttotal: 1m 23s\tremaining: 2m 38s\n345:\tlearn: 35.9454754\ttotal: 1m 23s\tremaining: 2m 38s\n346:\tlearn: 35.8061588\ttotal: 1m 23s\tremaining: 2m 37s\n347:\tlearn: 35.7233928\ttotal: 1m 24s\tremaining: 2m 37s\n348:\tlearn: 35.6611064\ttotal: 1m 24s\tremaining: 2m 37s\n349:\tlearn: 35.5533190\ttotal: 1m 24s\tremaining: 2m 37s\n350:\tlearn: 35.4858985\ttotal: 1m 24s\tremaining: 2m 36s\n351:\tlearn: 35.4064569\ttotal: 1m 25s\tremaining: 2m 36s\n352:\tlearn: 35.3206889\ttotal: 1m 25s\tremaining: 2m 36s\n353:\tlearn: 35.2380062\ttotal: 1m 25s\tremaining: 2m 36s\n354:\tlearn: 35.1762840\ttotal: 1m 25s\tremaining: 2m 36s\n355:\tlearn: 35.1012826\ttotal: 1m 26s\tremaining: 2m 35s\n356:\tlearn: 35.0503049\ttotal: 1m 26s\tremaining: 2m 35s\n357:\tlearn: 34.9364645\ttotal: 1m 26s\tremaining: 2m 35s\n358:\tlearn: 34.8549781\ttotal: 1m 26s\tremaining: 2m 35s\n359:\tlearn: 34.7495580\ttotal: 1m 27s\tremaining: 2m 34s\n360:\tlearn: 34.6274359\ttotal: 1m 27s\tremaining: 2m 34s\n361:\tlearn: 34.5237445\ttotal: 1m 27s\tremaining: 2m 34s\n362:\tlearn: 34.4022162\ttotal: 1m 27s\tremaining: 2m 34s\n363:\tlearn: 34.3042490\ttotal: 1m 28s\tremaining: 2m 33s\n364:\tlearn: 34.2867151\ttotal: 1m 28s\tremaining: 2m 33s\n365:\tlearn: 34.1618425\ttotal: 1m 28s\tremaining: 2m 33s\n366:\tlearn: 34.0825120\ttotal: 1m 29s\tremaining: 2m 33s\n367:\tlearn: 33.9909222\ttotal: 1m 29s\tremaining: 2m 33s\n368:\tlearn: 33.9350406\ttotal: 1m 29s\tremaining: 2m 33s\n369:\tlearn: 33.8002463\ttotal: 1m 29s\tremaining: 2m 32s\n370:\tlearn: 33.7274362\ttotal: 1m 29s\tremaining: 2m 32s\n371:\tlearn: 33.6619511\ttotal: 1m 30s\tremaining: 2m 32s\n372:\tlearn: 33.5570307\ttotal: 1m 30s\tremaining: 2m 32s\n373:\tlearn: 33.4645184\ttotal: 1m 30s\tremaining: 2m 31s\n374:\tlearn: 33.3608732\ttotal: 1m 30s\tremaining: 2m 31s\n375:\tlearn: 33.2567523\ttotal: 1m 31s\tremaining: 2m 31s\n376:\tlearn: 33.1832042\ttotal: 1m 31s\tremaining: 2m 31s\n377:\tlearn: 33.0030844\ttotal: 1m 31s\tremaining: 2m 30s\n378:\tlearn: 32.9264977\ttotal: 1m 31s\tremaining: 2m 30s\n379:\tlearn: 32.8595071\ttotal: 1m 32s\tremaining: 2m 30s\n380:\tlearn: 32.7730690\ttotal: 1m 32s\tremaining: 2m 30s\n381:\tlearn: 32.7056823\ttotal: 1m 32s\tremaining: 2m 29s\n382:\tlearn: 32.5855037\ttotal: 1m 32s\tremaining: 2m 29s\n383:\tlearn: 32.5356892\ttotal: 1m 33s\tremaining: 2m 29s\n384:\tlearn: 32.4563858\ttotal: 1m 33s\tremaining: 2m 29s\n385:\tlearn: 32.3814142\ttotal: 1m 33s\tremaining: 2m 28s\n386:\tlearn: 32.3042700\ttotal: 1m 33s\tremaining: 2m 28s\n387:\tlearn: 32.2119975\ttotal: 1m 33s\tremaining: 2m 28s\n388:\tlearn: 32.1322280\ttotal: 1m 34s\tremaining: 2m 28s\n389:\tlearn: 32.0350841\ttotal: 1m 34s\tremaining: 2m 27s\n390:\tlearn: 31.9642112\ttotal: 1m 34s\tremaining: 2m 27s\n391:\tlearn: 31.9064603\ttotal: 1m 34s\tremaining: 2m 27s\n392:\tlearn: 31.8281344\ttotal: 1m 35s\tremaining: 2m 27s\n393:\tlearn: 31.7683393\ttotal: 1m 35s\tremaining: 2m 26s\n394:\tlearn: 31.6962405\ttotal: 1m 35s\tremaining: 2m 26s\n395:\tlearn: 31.6459535\ttotal: 1m 35s\tremaining: 2m 26s\n396:\tlearn: 31.5631447\ttotal: 1m 36s\tremaining: 2m 26s\n397:\tlearn: 31.4931410\ttotal: 1m 36s\tremaining: 2m 25s\n398:\tlearn: 31.3981611\ttotal: 1m 36s\tremaining: 2m 25s\n399:\tlearn: 31.3429651\ttotal: 1m 36s\tremaining: 2m 25s\n400:\tlearn: 31.2667264\ttotal: 1m 37s\tremaining: 2m 25s\n401:\tlearn: 31.2172870\ttotal: 1m 37s\tremaining: 2m 24s\n402:\tlearn: 31.1718221\ttotal: 1m 37s\tremaining: 2m 24s\n403:\tlearn: 31.1012435\ttotal: 1m 37s\tremaining: 2m 24s\n404:\tlearn: 31.0642268\ttotal: 1m 38s\tremaining: 2m 24s\n405:\tlearn: 31.0082051\ttotal: 1m 38s\tremaining: 2m 23s\n406:\tlearn: 30.9612307\ttotal: 1m 38s\tremaining: 2m 23s\n407:\tlearn: 30.9131154\ttotal: 1m 38s\tremaining: 2m 23s\n408:\tlearn: 30.8172414\ttotal: 1m 39s\tremaining: 2m 23s\n409:\tlearn: 30.7658837\ttotal: 1m 39s\tremaining: 2m 22s\n410:\tlearn: 30.7463273\ttotal: 1m 39s\tremaining: 2m 22s\n411:\tlearn: 30.6321439\ttotal: 1m 39s\tremaining: 2m 22s\n412:\tlearn: 30.6076690\ttotal: 1m 39s\tremaining: 2m 22s\n413:\tlearn: 30.5169819\ttotal: 1m 40s\tremaining: 2m 21s\n414:\tlearn: 30.4658428\ttotal: 1m 40s\tremaining: 2m 21s\n415:\tlearn: 30.4154151\ttotal: 1m 40s\tremaining: 2m 21s\n416:\tlearn: 30.3711238\ttotal: 1m 40s\tremaining: 2m 21s\n417:\tlearn: 30.3531680\ttotal: 1m 41s\tremaining: 2m 20s\n418:\tlearn: 30.3394498\ttotal: 1m 41s\tremaining: 2m 20s\n419:\tlearn: 30.3165792\ttotal: 1m 41s\tremaining: 2m 20s\n420:\tlearn: 30.1871570\ttotal: 1m 41s\tremaining: 2m 20s\n421:\tlearn: 30.1434358\ttotal: 1m 42s\tremaining: 2m 19s\n422:\tlearn: 30.0431462\ttotal: 1m 42s\tremaining: 2m 19s\n423:\tlearn: 29.9579987\ttotal: 1m 42s\tremaining: 2m 19s\n424:\tlearn: 29.8621241\ttotal: 1m 42s\tremaining: 2m 19s\n425:\tlearn: 29.8375956\ttotal: 1m 43s\tremaining: 2m 18s\n426:\tlearn: 29.7065753\ttotal: 1m 43s\tremaining: 2m 18s\n427:\tlearn: 29.6504544\ttotal: 1m 43s\tremaining: 2m 18s\n428:\tlearn: 29.6292230\ttotal: 1m 43s\tremaining: 2m 18s\n429:\tlearn: 29.5073825\ttotal: 1m 43s\tremaining: 2m 17s\n430:\tlearn: 29.4849181\ttotal: 1m 44s\tremaining: 2m 17s\n431:\tlearn: 29.4338921\ttotal: 1m 44s\tremaining: 2m 17s\n432:\tlearn: 29.3333623\ttotal: 1m 44s\tremaining: 2m 17s\n433:\tlearn: 29.2386452\ttotal: 1m 44s\tremaining: 2m 16s\n434:\tlearn: 29.1996942\ttotal: 1m 45s\tremaining: 2m 16s\n435:\tlearn: 29.1355772\ttotal: 1m 45s\tremaining: 2m 16s\n436:\tlearn: 29.0528379\ttotal: 1m 45s\tremaining: 2m 16s\n437:\tlearn: 28.9979922\ttotal: 1m 45s\tremaining: 2m 15s\n438:\tlearn: 28.9033097\ttotal: 1m 46s\tremaining: 2m 15s\n439:\tlearn: 28.8056503\ttotal: 1m 46s\tremaining: 2m 15s\n440:\tlearn: 28.7139761\ttotal: 1m 46s\tremaining: 2m 15s\n441:\tlearn: 28.6112874\ttotal: 1m 46s\tremaining: 2m 14s\n442:\tlearn: 28.5489840\ttotal: 1m 47s\tremaining: 2m 14s\n443:\tlearn: 28.4481844\ttotal: 1m 47s\tremaining: 2m 14s\n444:\tlearn: 28.3578179\ttotal: 1m 47s\tremaining: 2m 14s\n445:\tlearn: 28.2566390\ttotal: 1m 47s\tremaining: 2m 13s\n446:\tlearn: 28.1805091\ttotal: 1m 48s\tremaining: 2m 13s\n447:\tlearn: 28.1040638\ttotal: 1m 48s\tremaining: 2m 13s\n448:\tlearn: 28.0546602\ttotal: 1m 48s\tremaining: 2m 13s\n449:\tlearn: 27.9610715\ttotal: 1m 48s\tremaining: 2m 12s\n450:\tlearn: 27.9030053\ttotal: 1m 49s\tremaining: 2m 12s\n451:\tlearn: 27.8229647\ttotal: 1m 49s\tremaining: 2m 12s\n452:\tlearn: 27.7810762\ttotal: 1m 49s\tremaining: 2m 12s\n453:\tlearn: 27.7419549\ttotal: 1m 49s\tremaining: 2m 11s\n454:\tlearn: 27.6956837\ttotal: 1m 50s\tremaining: 2m 11s\n455:\tlearn: 27.6161520\ttotal: 1m 50s\tremaining: 2m 11s\n456:\tlearn: 27.5469656\ttotal: 1m 50s\tremaining: 2m 11s\n457:\tlearn: 27.4664709\ttotal: 1m 50s\tremaining: 2m 11s\n458:\tlearn: 27.3919402\ttotal: 1m 51s\tremaining: 2m 11s\n459:\tlearn: 27.3703921\ttotal: 1m 51s\tremaining: 2m 10s\n460:\tlearn: 27.3321774\ttotal: 1m 51s\tremaining: 2m 10s\n461:\tlearn: 27.3101766\ttotal: 1m 51s\tremaining: 2m 10s\n462:\tlearn: 27.2390147\ttotal: 1m 52s\tremaining: 2m 10s\n463:\tlearn: 27.1798799\ttotal: 1m 52s\tremaining: 2m 9s\n464:\tlearn: 27.1495471\ttotal: 1m 52s\tremaining: 2m 9s\n465:\tlearn: 27.0745550\ttotal: 1m 52s\tremaining: 2m 9s\n466:\tlearn: 26.9528525\ttotal: 1m 53s\tremaining: 2m 9s\n467:\tlearn: 26.8448333\ttotal: 1m 53s\tremaining: 2m 8s\n468:\tlearn: 26.7593979\ttotal: 1m 53s\tremaining: 2m 8s\n469:\tlearn: 26.7030785\ttotal: 1m 53s\tremaining: 2m 8s\n470:\tlearn: 26.6509109\ttotal: 1m 54s\tremaining: 2m 8s\n471:\tlearn: 26.5897037\ttotal: 1m 54s\tremaining: 2m 7s\n472:\tlearn: 26.5040256\ttotal: 1m 54s\tremaining: 2m 7s\n473:\tlearn: 26.4272921\ttotal: 1m 54s\tremaining: 2m 7s\n474:\tlearn: 26.3162674\ttotal: 1m 55s\tremaining: 2m 7s\n475:\tlearn: 26.2210309\ttotal: 1m 55s\tremaining: 2m 6s\n476:\tlearn: 26.1367321\ttotal: 1m 55s\tremaining: 2m 6s\n477:\tlearn: 26.0537386\ttotal: 1m 55s\tremaining: 2m 6s\n478:\tlearn: 25.9923331\ttotal: 1m 56s\tremaining: 2m 6s\n479:\tlearn: 25.9005049\ttotal: 1m 56s\tremaining: 2m 5s\n480:\tlearn: 25.8448168\ttotal: 1m 56s\tremaining: 2m 5s\n481:\tlearn: 25.7892992\ttotal: 1m 56s\tremaining: 2m 5s\n482:\tlearn: 25.7182565\ttotal: 1m 56s\tremaining: 2m 5s\n483:\tlearn: 25.6320866\ttotal: 1m 57s\tremaining: 2m 4s\n484:\tlearn: 25.5672881\ttotal: 1m 57s\tremaining: 2m 4s\n485:\tlearn: 25.5309841\ttotal: 1m 57s\tremaining: 2m 4s\n486:\tlearn: 25.4617532\ttotal: 1m 57s\tremaining: 2m 4s\n487:\tlearn: 25.4007470\ttotal: 1m 58s\tremaining: 2m 3s\n488:\tlearn: 25.3408066\ttotal: 1m 58s\tremaining: 2m 3s\n489:\tlearn: 25.3039520\ttotal: 1m 58s\tremaining: 2m 3s\n490:\tlearn: 25.2411412\ttotal: 1m 58s\tremaining: 2m 3s\n491:\tlearn: 25.2311491\ttotal: 1m 59s\tremaining: 2m 2s\n492:\tlearn: 25.1906085\ttotal: 1m 59s\tremaining: 2m 2s\n493:\tlearn: 25.1112937\ttotal: 1m 59s\tremaining: 2m 2s\n494:\tlearn: 25.0379843\ttotal: 1m 59s\tremaining: 2m 2s\n495:\tlearn: 24.9579674\ttotal: 2m\tremaining: 2m 1s\n496:\tlearn: 24.9021351\ttotal: 2m\tremaining: 2m 1s\n497:\tlearn: 24.8820167\ttotal: 2m\tremaining: 2m 1s\n498:\tlearn: 24.8136355\ttotal: 2m\tremaining: 2m 1s\n499:\tlearn: 24.7501503\ttotal: 2m 1s\tremaining: 2m 1s\n500:\tlearn: 24.7033197\ttotal: 2m 1s\tremaining: 2m\n501:\tlearn: 24.6323600\ttotal: 2m 1s\tremaining: 2m\n502:\tlearn: 24.5536646\ttotal: 2m 1s\tremaining: 2m\n503:\tlearn: 24.4930442\ttotal: 2m 1s\tremaining: 2m\n504:\tlearn: 24.4447277\ttotal: 2m 2s\tremaining: 1m 59s\n505:\tlearn: 24.3894535\ttotal: 2m 2s\tremaining: 1m 59s\n506:\tlearn: 24.3085913\ttotal: 2m 2s\tremaining: 1m 59s\n507:\tlearn: 24.2321632\ttotal: 2m 2s\tremaining: 1m 59s\n508:\tlearn: 24.1591729\ttotal: 2m 3s\tremaining: 1m 58s\n509:\tlearn: 24.1242516\ttotal: 2m 3s\tremaining: 1m 58s\n510:\tlearn: 24.0405893\ttotal: 2m 3s\tremaining: 1m 58s\n511:\tlearn: 23.9765077\ttotal: 2m 3s\tremaining: 1m 58s\n512:\tlearn: 23.9283321\ttotal: 2m 4s\tremaining: 1m 57s\n513:\tlearn: 23.8798178\ttotal: 2m 4s\tremaining: 1m 57s\n514:\tlearn: 23.8488735\ttotal: 2m 4s\tremaining: 1m 57s\n515:\tlearn: 23.7955122\ttotal: 2m 4s\tremaining: 1m 57s\n516:\tlearn: 23.7858428\ttotal: 2m 5s\tremaining: 1m 56s\n517:\tlearn: 23.7215012\ttotal: 2m 5s\tremaining: 1m 56s\n518:\tlearn: 23.6919083\ttotal: 2m 5s\tremaining: 1m 56s\n519:\tlearn: 23.6731406\ttotal: 2m 5s\tremaining: 1m 56s\n520:\tlearn: 23.6250627\ttotal: 2m 6s\tremaining: 1m 55s\n521:\tlearn: 23.5359337\ttotal: 2m 6s\tremaining: 1m 55s\n522:\tlearn: 23.5111217\ttotal: 2m 6s\tremaining: 1m 55s\n523:\tlearn: 23.4667096\ttotal: 2m 6s\tremaining: 1m 55s\n524:\tlearn: 23.3867026\ttotal: 2m 7s\tremaining: 1m 54s\n525:\tlearn: 23.3554859\ttotal: 2m 7s\tremaining: 1m 54s\n526:\tlearn: 23.3410268\ttotal: 2m 7s\tremaining: 1m 54s\n527:\tlearn: 23.2778972\ttotal: 2m 7s\tremaining: 1m 54s\n528:\tlearn: 23.2315919\ttotal: 2m 7s\tremaining: 1m 53s\n529:\tlearn: 23.1463667\ttotal: 2m 8s\tremaining: 1m 53s\n530:\tlearn: 23.1182465\ttotal: 2m 8s\tremaining: 1m 53s\n531:\tlearn: 23.0792569\ttotal: 2m 8s\tremaining: 1m 53s\n532:\tlearn: 22.9895005\ttotal: 2m 8s\tremaining: 1m 52s\n533:\tlearn: 22.9664324\ttotal: 2m 9s\tremaining: 1m 52s\n534:\tlearn: 22.9183014\ttotal: 2m 9s\tremaining: 1m 52s\n535:\tlearn: 22.8577141\ttotal: 2m 9s\tremaining: 1m 52s\n536:\tlearn: 22.7880304\ttotal: 2m 9s\tremaining: 1m 51s\n537:\tlearn: 22.7137095\ttotal: 2m 10s\tremaining: 1m 51s\n538:\tlearn: 22.6184343\ttotal: 2m 10s\tremaining: 1m 51s\n539:\tlearn: 22.5353405\ttotal: 2m 10s\tremaining: 1m 51s\n540:\tlearn: 22.4508672\ttotal: 2m 10s\tremaining: 1m 50s\n541:\tlearn: 22.4171416\ttotal: 2m 11s\tremaining: 1m 50s\n542:\tlearn: 22.3859928\ttotal: 2m 11s\tremaining: 1m 50s\n543:\tlearn: 22.3601251\ttotal: 2m 11s\tremaining: 1m 50s\n544:\tlearn: 22.2837261\ttotal: 2m 11s\tremaining: 1m 50s\n545:\tlearn: 22.2262369\ttotal: 2m 12s\tremaining: 1m 49s\n546:\tlearn: 22.1836739\ttotal: 2m 12s\tremaining: 1m 49s\n547:\tlearn: 22.0974834\ttotal: 2m 12s\tremaining: 1m 49s\n548:\tlearn: 22.0731101\ttotal: 2m 12s\tremaining: 1m 49s\n549:\tlearn: 22.0071882\ttotal: 2m 12s\tremaining: 1m 48s\n550:\tlearn: 21.9437140\ttotal: 2m 13s\tremaining: 1m 48s\n551:\tlearn: 21.8776585\ttotal: 2m 13s\tremaining: 1m 48s\n552:\tlearn: 21.8350003\ttotal: 2m 13s\tremaining: 1m 48s\n553:\tlearn: 21.7508118\ttotal: 2m 13s\tremaining: 1m 47s\n554:\tlearn: 21.7126049\ttotal: 2m 14s\tremaining: 1m 47s\n555:\tlearn: 21.6542535\ttotal: 2m 14s\tremaining: 1m 47s\n556:\tlearn: 21.6271401\ttotal: 2m 14s\tremaining: 1m 47s\n557:\tlearn: 21.5592392\ttotal: 2m 14s\tremaining: 1m 46s\n558:\tlearn: 21.5142238\ttotal: 2m 15s\tremaining: 1m 46s\n559:\tlearn: 21.4820908\ttotal: 2m 15s\tremaining: 1m 46s\n560:\tlearn: 21.4464026\ttotal: 2m 15s\tremaining: 1m 46s\n561:\tlearn: 21.4209511\ttotal: 2m 15s\tremaining: 1m 45s\n562:\tlearn: 21.4125275\ttotal: 2m 16s\tremaining: 1m 45s\n563:\tlearn: 21.3758149\ttotal: 2m 16s\tremaining: 1m 45s\n564:\tlearn: 21.3326433\ttotal: 2m 16s\tremaining: 1m 45s\n565:\tlearn: 21.2588382\ttotal: 2m 16s\tremaining: 1m 44s\n566:\tlearn: 21.2332884\ttotal: 2m 17s\tremaining: 1m 44s\n567:\tlearn: 21.1993728\ttotal: 2m 17s\tremaining: 1m 44s\n568:\tlearn: 21.1495424\ttotal: 2m 17s\tremaining: 1m 44s\n569:\tlearn: 21.0774827\ttotal: 2m 17s\tremaining: 1m 43s\n570:\tlearn: 21.0176053\ttotal: 2m 18s\tremaining: 1m 43s\n571:\tlearn: 20.9551151\ttotal: 2m 18s\tremaining: 1m 43s\n572:\tlearn: 20.9141002\ttotal: 2m 18s\tremaining: 1m 43s\n573:\tlearn: 20.8764889\ttotal: 2m 18s\tremaining: 1m 42s\n574:\tlearn: 20.8267213\ttotal: 2m 18s\tremaining: 1m 42s\n575:\tlearn: 20.7769647\ttotal: 2m 19s\tremaining: 1m 42s\n576:\tlearn: 20.6998893\ttotal: 2m 19s\tremaining: 1m 42s\n577:\tlearn: 20.6563902\ttotal: 2m 19s\tremaining: 1m 42s\n578:\tlearn: 20.6216633\ttotal: 2m 19s\tremaining: 1m 41s\n579:\tlearn: 20.5933862\ttotal: 2m 20s\tremaining: 1m 41s\n580:\tlearn: 20.5533444\ttotal: 2m 20s\tremaining: 1m 41s\n581:\tlearn: 20.5165955\ttotal: 2m 20s\tremaining: 1m 41s\n582:\tlearn: 20.4685705\ttotal: 2m 20s\tremaining: 1m 40s\n583:\tlearn: 20.4445397\ttotal: 2m 21s\tremaining: 1m 40s\n584:\tlearn: 20.4043120\ttotal: 2m 21s\tremaining: 1m 40s\n585:\tlearn: 20.3799358\ttotal: 2m 21s\tremaining: 1m 40s\n586:\tlearn: 20.3079572\ttotal: 2m 22s\tremaining: 1m 39s\n587:\tlearn: 20.2375080\ttotal: 2m 22s\tremaining: 1m 39s\n588:\tlearn: 20.2036047\ttotal: 2m 22s\tremaining: 1m 39s\n589:\tlearn: 20.1947192\ttotal: 2m 22s\tremaining: 1m 39s\n590:\tlearn: 20.1537265\ttotal: 2m 23s\tremaining: 1m 39s\n591:\tlearn: 20.1376701\ttotal: 2m 23s\tremaining: 1m 38s\n592:\tlearn: 20.1000990\ttotal: 2m 23s\tremaining: 1m 38s\n593:\tlearn: 20.0867200\ttotal: 2m 23s\tremaining: 1m 38s\n594:\tlearn: 20.0379570\ttotal: 2m 24s\tremaining: 1m 38s\n595:\tlearn: 19.9937849\ttotal: 2m 24s\tremaining: 1m 37s\n596:\tlearn: 19.9199837\ttotal: 2m 24s\tremaining: 1m 37s\n597:\tlearn: 19.8947086\ttotal: 2m 24s\tremaining: 1m 37s\n598:\tlearn: 19.8174054\ttotal: 2m 25s\tremaining: 1m 37s\n599:\tlearn: 19.7645478\ttotal: 2m 25s\tremaining: 1m 36s\n600:\tlearn: 19.7276295\ttotal: 2m 25s\tremaining: 1m 36s\n601:\tlearn: 19.6986566\ttotal: 2m 25s\tremaining: 1m 36s\n602:\tlearn: 19.6581696\ttotal: 2m 25s\tremaining: 1m 36s\n603:\tlearn: 19.6067492\ttotal: 2m 26s\tremaining: 1m 35s\n604:\tlearn: 19.5602275\ttotal: 2m 26s\tremaining: 1m 35s\n605:\tlearn: 19.4977813\ttotal: 2m 26s\tremaining: 1m 35s\n606:\tlearn: 19.4380334\ttotal: 2m 26s\tremaining: 1m 35s\n607:\tlearn: 19.3760451\ttotal: 2m 27s\tremaining: 1m 34s\n608:\tlearn: 19.3685041\ttotal: 2m 27s\tremaining: 1m 34s\n609:\tlearn: 19.2948882\ttotal: 2m 27s\tremaining: 1m 34s\n610:\tlearn: 19.2459973\ttotal: 2m 27s\tremaining: 1m 34s\n611:\tlearn: 19.1857480\ttotal: 2m 28s\tremaining: 1m 33s\n612:\tlearn: 19.1485006\ttotal: 2m 28s\tremaining: 1m 33s\n613:\tlearn: 19.1223823\ttotal: 2m 28s\tremaining: 1m 33s\n614:\tlearn: 19.0687720\ttotal: 2m 28s\tremaining: 1m 33s\n615:\tlearn: 19.0503097\ttotal: 2m 29s\tremaining: 1m 32s\n616:\tlearn: 18.9793825\ttotal: 2m 29s\tremaining: 1m 32s\n617:\tlearn: 18.9250641\ttotal: 2m 29s\tremaining: 1m 32s\n618:\tlearn: 18.8729948\ttotal: 2m 29s\tremaining: 1m 32s\n619:\tlearn: 18.8282491\ttotal: 2m 30s\tremaining: 1m 31s\n620:\tlearn: 18.7858796\ttotal: 2m 30s\tremaining: 1m 31s\n621:\tlearn: 18.7475357\ttotal: 2m 30s\tremaining: 1m 31s\n622:\tlearn: 18.6876836\ttotal: 2m 30s\tremaining: 1m 31s\n623:\tlearn: 18.6414628\ttotal: 2m 30s\tremaining: 1m 30s\n624:\tlearn: 18.5858944\ttotal: 2m 31s\tremaining: 1m 30s\n625:\tlearn: 18.5323389\ttotal: 2m 31s\tremaining: 1m 30s\n626:\tlearn: 18.4763788\ttotal: 2m 31s\tremaining: 1m 30s\n627:\tlearn: 18.4352524\ttotal: 2m 31s\tremaining: 1m 30s\n628:\tlearn: 18.3804336\ttotal: 2m 32s\tremaining: 1m 29s\n629:\tlearn: 18.3225433\ttotal: 2m 32s\tremaining: 1m 29s\n630:\tlearn: 18.2635240\ttotal: 2m 32s\tremaining: 1m 29s\n631:\tlearn: 18.2209383\ttotal: 2m 32s\tremaining: 1m 29s\n632:\tlearn: 18.1662180\ttotal: 2m 33s\tremaining: 1m 28s\n633:\tlearn: 18.1026163\ttotal: 2m 33s\tremaining: 1m 28s\n634:\tlearn: 18.0618893\ttotal: 2m 33s\tremaining: 1m 28s\n635:\tlearn: 18.0163559\ttotal: 2m 33s\tremaining: 1m 28s\n636:\tlearn: 17.9660517\ttotal: 2m 34s\tremaining: 1m 27s\n637:\tlearn: 17.9389494\ttotal: 2m 34s\tremaining: 1m 27s\n638:\tlearn: 17.8879514\ttotal: 2m 34s\tremaining: 1m 27s\n639:\tlearn: 17.8495480\ttotal: 2m 34s\tremaining: 1m 27s\n640:\tlearn: 17.8069168\ttotal: 2m 35s\tremaining: 1m 26s\n641:\tlearn: 17.7613171\ttotal: 2m 35s\tremaining: 1m 26s\n642:\tlearn: 17.7457534\ttotal: 2m 35s\tremaining: 1m 26s\n643:\tlearn: 17.7198718\ttotal: 2m 35s\tremaining: 1m 26s\n644:\tlearn: 17.7094841\ttotal: 2m 36s\tremaining: 1m 25s\n645:\tlearn: 17.6725152\ttotal: 2m 36s\tremaining: 1m 25s\n646:\tlearn: 17.6452524\ttotal: 2m 36s\tremaining: 1m 25s\n647:\tlearn: 17.6027226\ttotal: 2m 36s\tremaining: 1m 25s\n648:\tlearn: 17.5497697\ttotal: 2m 37s\tremaining: 1m 24s\n649:\tlearn: 17.5202715\ttotal: 2m 37s\tremaining: 1m 24s\n650:\tlearn: 17.4776739\ttotal: 2m 37s\tremaining: 1m 24s\n651:\tlearn: 17.4357825\ttotal: 2m 37s\tremaining: 1m 24s\n652:\tlearn: 17.3790843\ttotal: 2m 37s\tremaining: 1m 23s\n653:\tlearn: 17.3378679\ttotal: 2m 38s\tremaining: 1m 23s\n654:\tlearn: 17.3126263\ttotal: 2m 38s\tremaining: 1m 23s\n655:\tlearn: 17.2838882\ttotal: 2m 38s\tremaining: 1m 23s\n656:\tlearn: 17.2302853\ttotal: 2m 38s\tremaining: 1m 22s\n657:\tlearn: 17.1805144\ttotal: 2m 39s\tremaining: 1m 22s\n658:\tlearn: 17.1312485\ttotal: 2m 39s\tremaining: 1m 22s\n659:\tlearn: 17.0670937\ttotal: 2m 39s\tremaining: 1m 22s\n660:\tlearn: 17.0056882\ttotal: 2m 39s\tremaining: 1m 22s\n661:\tlearn: 16.9502692\ttotal: 2m 40s\tremaining: 1m 21s\n662:\tlearn: 16.9178928\ttotal: 2m 40s\tremaining: 1m 21s\n663:\tlearn: 16.8628903\ttotal: 2m 40s\tremaining: 1m 21s\n664:\tlearn: 16.8239040\ttotal: 2m 40s\tremaining: 1m 21s\n665:\tlearn: 16.7919311\ttotal: 2m 41s\tremaining: 1m 20s\n666:\tlearn: 16.7606556\ttotal: 2m 41s\tremaining: 1m 20s\n667:\tlearn: 16.7102893\ttotal: 2m 41s\tremaining: 1m 20s\n668:\tlearn: 16.6661388\ttotal: 2m 41s\tremaining: 1m 20s\n669:\tlearn: 16.6603137\ttotal: 2m 42s\tremaining: 1m 19s\n670:\tlearn: 16.6163976\ttotal: 2m 42s\tremaining: 1m 19s\n671:\tlearn: 16.5592789\ttotal: 2m 42s\tremaining: 1m 19s\n672:\tlearn: 16.5485713\ttotal: 2m 42s\tremaining: 1m 19s\n673:\tlearn: 16.4801788\ttotal: 2m 43s\tremaining: 1m 18s\n674:\tlearn: 16.4328315\ttotal: 2m 43s\tremaining: 1m 18s\n675:\tlearn: 16.3817588\ttotal: 2m 43s\tremaining: 1m 18s\n676:\tlearn: 16.3434442\ttotal: 2m 43s\tremaining: 1m 18s\n677:\tlearn: 16.3174873\ttotal: 2m 43s\tremaining: 1m 17s\n678:\tlearn: 16.2714860\ttotal: 2m 44s\tremaining: 1m 17s\n679:\tlearn: 16.2210800\ttotal: 2m 44s\tremaining: 1m 17s\n680:\tlearn: 16.1686951\ttotal: 2m 44s\tremaining: 1m 17s\n681:\tlearn: 16.1432999\ttotal: 2m 44s\tremaining: 1m 16s\n682:\tlearn: 16.1004647\ttotal: 2m 45s\tremaining: 1m 16s\n683:\tlearn: 16.0568590\ttotal: 2m 45s\tremaining: 1m 16s\n684:\tlearn: 16.0147539\ttotal: 2m 45s\tremaining: 1m 16s\n685:\tlearn: 15.9600488\ttotal: 2m 45s\tremaining: 1m 15s\n686:\tlearn: 15.9289792\ttotal: 2m 46s\tremaining: 1m 15s\n687:\tlearn: 15.8861450\ttotal: 2m 46s\tremaining: 1m 15s\n688:\tlearn: 15.8557856\ttotal: 2m 46s\tremaining: 1m 15s\n689:\tlearn: 15.8273081\ttotal: 2m 46s\tremaining: 1m 15s\n690:\tlearn: 15.7820990\ttotal: 2m 47s\tremaining: 1m 14s\n691:\tlearn: 15.7332888\ttotal: 2m 47s\tremaining: 1m 14s\n692:\tlearn: 15.6979460\ttotal: 2m 47s\tremaining: 1m 14s\n693:\tlearn: 15.6602372\ttotal: 2m 47s\tremaining: 1m 14s\n694:\tlearn: 15.6259562\ttotal: 2m 48s\tremaining: 1m 13s\n695:\tlearn: 15.5753257\ttotal: 2m 48s\tremaining: 1m 13s\n696:\tlearn: 15.5406226\ttotal: 2m 48s\tremaining: 1m 13s\n697:\tlearn: 15.5117503\ttotal: 2m 48s\tremaining: 1m 13s\n698:\tlearn: 15.4687545\ttotal: 2m 49s\tremaining: 1m 12s\n699:\tlearn: 15.4252577\ttotal: 2m 49s\tremaining: 1m 12s\n700:\tlearn: 15.3902529\ttotal: 2m 49s\tremaining: 1m 12s\n701:\tlearn: 15.3497743\ttotal: 2m 49s\tremaining: 1m 12s\n702:\tlearn: 15.2960217\ttotal: 2m 50s\tremaining: 1m 11s\n703:\tlearn: 15.2783733\ttotal: 2m 50s\tremaining: 1m 11s\n704:\tlearn: 15.2697228\ttotal: 2m 50s\tremaining: 1m 11s\n705:\tlearn: 15.2396740\ttotal: 2m 50s\tremaining: 1m 11s\n706:\tlearn: 15.2106064\ttotal: 2m 51s\tremaining: 1m 10s\n707:\tlearn: 15.1744935\ttotal: 2m 51s\tremaining: 1m 10s\n708:\tlearn: 15.1651461\ttotal: 2m 51s\tremaining: 1m 10s\n709:\tlearn: 15.1158235\ttotal: 2m 51s\tremaining: 1m 10s\n710:\tlearn: 15.0872877\ttotal: 2m 51s\tremaining: 1m 9s\n711:\tlearn: 15.0435008\ttotal: 2m 52s\tremaining: 1m 9s\n712:\tlearn: 15.0002293\ttotal: 2m 52s\tremaining: 1m 9s\n713:\tlearn: 14.9618561\ttotal: 2m 52s\tremaining: 1m 9s\n714:\tlearn: 14.9347363\ttotal: 2m 53s\tremaining: 1m 9s\n715:\tlearn: 14.8937878\ttotal: 2m 53s\tremaining: 1m 8s\n716:\tlearn: 14.8663759\ttotal: 2m 53s\tremaining: 1m 8s\n717:\tlearn: 14.8452339\ttotal: 2m 53s\tremaining: 1m 8s\n718:\tlearn: 14.7993632\ttotal: 2m 54s\tremaining: 1m 8s\n719:\tlearn: 14.7625800\ttotal: 2m 54s\tremaining: 1m 7s\n720:\tlearn: 14.7198976\ttotal: 2m 54s\tremaining: 1m 7s\n721:\tlearn: 14.7109144\ttotal: 2m 54s\tremaining: 1m 7s\n722:\tlearn: 14.6810647\ttotal: 2m 55s\tremaining: 1m 7s\n723:\tlearn: 14.6439991\ttotal: 2m 55s\tremaining: 1m 6s\n724:\tlearn: 14.6072165\ttotal: 2m 55s\tremaining: 1m 6s\n725:\tlearn: 14.5798179\ttotal: 2m 55s\tremaining: 1m 6s\n726:\tlearn: 14.5459798\ttotal: 2m 56s\tremaining: 1m 6s\n727:\tlearn: 14.5189614\ttotal: 2m 56s\tremaining: 1m 5s\n728:\tlearn: 14.4816915\ttotal: 2m 56s\tremaining: 1m 5s\n729:\tlearn: 14.4519031\ttotal: 2m 56s\tremaining: 1m 5s\n730:\tlearn: 14.4450488\ttotal: 2m 57s\tremaining: 1m 5s\n731:\tlearn: 14.4132506\ttotal: 2m 57s\tremaining: 1m 4s\n732:\tlearn: 14.3950155\ttotal: 2m 57s\tremaining: 1m 4s\n733:\tlearn: 14.3536566\ttotal: 2m 57s\tremaining: 1m 4s\n734:\tlearn: 14.3222776\ttotal: 2m 58s\tremaining: 1m 4s\n735:\tlearn: 14.2837140\ttotal: 2m 58s\tremaining: 1m 3s\n736:\tlearn: 14.2518204\ttotal: 2m 58s\tremaining: 1m 3s\n737:\tlearn: 14.2458756\ttotal: 2m 58s\tremaining: 1m 3s\n738:\tlearn: 14.2157451\ttotal: 2m 58s\tremaining: 1m 3s\n739:\tlearn: 14.1945216\ttotal: 2m 59s\tremaining: 1m 2s\n740:\tlearn: 14.1702373\ttotal: 2m 59s\tremaining: 1m 2s\n741:\tlearn: 14.1415101\ttotal: 2m 59s\tremaining: 1m 2s\n742:\tlearn: 14.1197595\ttotal: 2m 59s\tremaining: 1m 2s\n743:\tlearn: 14.1025771\ttotal: 3m\tremaining: 1m 1s\n744:\tlearn: 14.0700518\ttotal: 3m\tremaining: 1m 1s\n745:\tlearn: 14.0282337\ttotal: 3m\tremaining: 1m 1s\n746:\tlearn: 14.0014403\ttotal: 3m\tremaining: 1m 1s\n747:\tlearn: 13.9698105\ttotal: 3m 1s\tremaining: 1m 1s\n748:\tlearn: 13.9568751\ttotal: 3m 1s\tremaining: 1m\n749:\tlearn: 13.9507074\ttotal: 3m 1s\tremaining: 1m\n750:\tlearn: 13.9223091\ttotal: 3m 1s\tremaining: 1m\n751:\tlearn: 13.8713931\ttotal: 3m 2s\tremaining: 1m\n752:\tlearn: 13.8517766\ttotal: 3m 2s\tremaining: 59.8s\n753:\tlearn: 13.8148129\ttotal: 3m 2s\tremaining: 59.6s\n754:\tlearn: 13.7757363\ttotal: 3m 2s\tremaining: 59.3s\n755:\tlearn: 13.7318066\ttotal: 3m 3s\tremaining: 59.1s\n756:\tlearn: 13.7215332\ttotal: 3m 3s\tremaining: 58.8s\n757:\tlearn: 13.6865399\ttotal: 3m 3s\tremaining: 58.6s\n758:\tlearn: 13.6378929\ttotal: 3m 3s\tremaining: 58.4s\n759:\tlearn: 13.6105584\ttotal: 3m 4s\tremaining: 58.1s\n760:\tlearn: 13.5875403\ttotal: 3m 4s\tremaining: 57.9s\n761:\tlearn: 13.5599331\ttotal: 3m 4s\tremaining: 57.6s\n762:\tlearn: 13.5315215\ttotal: 3m 4s\tremaining: 57.4s\n763:\tlearn: 13.5140054\ttotal: 3m 5s\tremaining: 57.1s\n764:\tlearn: 13.5091489\ttotal: 3m 5s\tremaining: 56.9s\n765:\tlearn: 13.4703161\ttotal: 3m 5s\tremaining: 56.7s\n766:\tlearn: 13.4431432\ttotal: 3m 5s\tremaining: 56.4s\n767:\tlearn: 13.4135399\ttotal: 3m 5s\tremaining: 56.2s\n768:\tlearn: 13.3427936\ttotal: 3m 6s\tremaining: 55.9s\n769:\tlearn: 13.3235814\ttotal: 3m 6s\tremaining: 55.7s\n770:\tlearn: 13.2941968\ttotal: 3m 6s\tremaining: 55.4s\n771:\tlearn: 13.2907190\ttotal: 3m 6s\tremaining: 55.2s\n772:\tlearn: 13.2467539\ttotal: 3m 7s\tremaining: 55s\n773:\tlearn: 13.2039738\ttotal: 3m 7s\tremaining: 54.7s\n774:\tlearn: 13.1939923\ttotal: 3m 7s\tremaining: 54.5s\n775:\tlearn: 13.1617754\ttotal: 3m 7s\tremaining: 54.2s\n776:\tlearn: 13.1454412\ttotal: 3m 8s\tremaining: 54s\n777:\tlearn: 13.0977838\ttotal: 3m 8s\tremaining: 53.7s\n778:\tlearn: 13.0788193\ttotal: 3m 8s\tremaining: 53.5s\n779:\tlearn: 13.0427193\ttotal: 3m 8s\tremaining: 53.3s\n780:\tlearn: 13.0181602\ttotal: 3m 9s\tremaining: 53s\n781:\tlearn: 12.9882226\ttotal: 3m 9s\tremaining: 52.8s\n782:\tlearn: 12.9609121\ttotal: 3m 9s\tremaining: 52.5s\n783:\tlearn: 12.9556411\ttotal: 3m 9s\tremaining: 52.3s\n784:\tlearn: 12.9175087\ttotal: 3m 10s\tremaining: 52s\n785:\tlearn: 12.8775104\ttotal: 3m 10s\tremaining: 51.8s\n786:\tlearn: 12.8424105\ttotal: 3m 10s\tremaining: 51.6s\n787:\tlearn: 12.8096175\ttotal: 3m 10s\tremaining: 51.3s\n788:\tlearn: 12.7901857\ttotal: 3m 10s\tremaining: 51.1s\n789:\tlearn: 12.7549150\ttotal: 3m 11s\tremaining: 50.8s\n790:\tlearn: 12.7266420\ttotal: 3m 11s\tremaining: 50.6s\n791:\tlearn: 12.7038402\ttotal: 3m 11s\tremaining: 50.3s\n792:\tlearn: 12.6554238\ttotal: 3m 11s\tremaining: 50.1s\n793:\tlearn: 12.6255691\ttotal: 3m 12s\tremaining: 49.9s\n794:\tlearn: 12.6163411\ttotal: 3m 12s\tremaining: 49.6s\n795:\tlearn: 12.5784605\ttotal: 3m 12s\tremaining: 49.4s\n796:\tlearn: 12.5595379\ttotal: 3m 12s\tremaining: 49.1s\n797:\tlearn: 12.5273873\ttotal: 3m 13s\tremaining: 48.9s\n798:\tlearn: 12.4962863\ttotal: 3m 13s\tremaining: 48.6s\n799:\tlearn: 12.4803623\ttotal: 3m 13s\tremaining: 48.4s\n800:\tlearn: 12.4426977\ttotal: 3m 13s\tremaining: 48.2s\n801:\tlearn: 12.4111310\ttotal: 3m 14s\tremaining: 47.9s\n802:\tlearn: 12.3960698\ttotal: 3m 14s\tremaining: 47.7s\n803:\tlearn: 12.3665014\ttotal: 3m 14s\tremaining: 47.4s\n804:\tlearn: 12.3349858\ttotal: 3m 14s\tremaining: 47.2s\n805:\tlearn: 12.3156189\ttotal: 3m 15s\tremaining: 46.9s\n806:\tlearn: 12.2807154\ttotal: 3m 15s\tremaining: 46.7s\n807:\tlearn: 12.2503978\ttotal: 3m 15s\tremaining: 46.5s\n808:\tlearn: 12.2254466\ttotal: 3m 15s\tremaining: 46.2s\n809:\tlearn: 12.1846272\ttotal: 3m 16s\tremaining: 46s\n810:\tlearn: 12.1499411\ttotal: 3m 16s\tremaining: 45.7s\n811:\tlearn: 12.1250675\ttotal: 3m 16s\tremaining: 45.5s\n812:\tlearn: 12.0870345\ttotal: 3m 16s\tremaining: 45.2s\n813:\tlearn: 12.0771517\ttotal: 3m 16s\tremaining: 45s\n814:\tlearn: 12.0333551\ttotal: 3m 17s\tremaining: 44.8s\n815:\tlearn: 12.0019423\ttotal: 3m 17s\tremaining: 44.5s\n816:\tlearn: 11.9838934\ttotal: 3m 17s\tremaining: 44.3s\n817:\tlearn: 11.9664032\ttotal: 3m 17s\tremaining: 44s\n818:\tlearn: 11.9621648\ttotal: 3m 18s\tremaining: 43.8s\n819:\tlearn: 11.9435139\ttotal: 3m 18s\tremaining: 43.5s\n820:\tlearn: 11.9310879\ttotal: 3m 18s\tremaining: 43.3s\n821:\tlearn: 11.9056319\ttotal: 3m 18s\tremaining: 43.1s\n822:\tlearn: 11.8704311\ttotal: 3m 19s\tremaining: 42.8s\n823:\tlearn: 11.8402757\ttotal: 3m 19s\tremaining: 42.6s\n824:\tlearn: 11.8076406\ttotal: 3m 19s\tremaining: 42.3s\n825:\tlearn: 11.7952876\ttotal: 3m 19s\tremaining: 42.1s\n826:\tlearn: 11.7717044\ttotal: 3m 20s\tremaining: 41.8s\n827:\tlearn: 11.7614337\ttotal: 3m 20s\tremaining: 41.6s\n828:\tlearn: 11.7176747\ttotal: 3m 20s\tremaining: 41.4s\n829:\tlearn: 11.6899739\ttotal: 3m 20s\tremaining: 41.1s\n830:\tlearn: 11.6628600\ttotal: 3m 21s\tremaining: 40.9s\n831:\tlearn: 11.6452036\ttotal: 3m 21s\tremaining: 40.6s\n832:\tlearn: 11.6226268\ttotal: 3m 21s\tremaining: 40.4s\n833:\tlearn: 11.5844349\ttotal: 3m 21s\tremaining: 40.1s\n834:\tlearn: 11.5540991\ttotal: 3m 21s\tremaining: 39.9s\n835:\tlearn: 11.5223562\ttotal: 3m 22s\tremaining: 39.7s\n836:\tlearn: 11.5037254\ttotal: 3m 22s\tremaining: 39.4s\n837:\tlearn: 11.4808579\ttotal: 3m 22s\tremaining: 39.2s\n838:\tlearn: 11.4500135\ttotal: 3m 22s\tremaining: 38.9s\n839:\tlearn: 11.4106475\ttotal: 3m 23s\tremaining: 38.7s\n840:\tlearn: 11.3948751\ttotal: 3m 23s\tremaining: 38.5s\n841:\tlearn: 11.3634014\ttotal: 3m 23s\tremaining: 38.2s\n842:\tlearn: 11.3340730\ttotal: 3m 24s\tremaining: 38s\n843:\tlearn: 11.3276251\ttotal: 3m 24s\tremaining: 37.8s\n844:\tlearn: 11.3213499\ttotal: 3m 24s\tremaining: 37.5s\n845:\tlearn: 11.3003686\ttotal: 3m 24s\tremaining: 37.3s\n846:\tlearn: 11.2841412\ttotal: 3m 25s\tremaining: 37.1s\n847:\tlearn: 11.2615543\ttotal: 3m 25s\tremaining: 36.8s\n848:\tlearn: 11.2350391\ttotal: 3m 25s\tremaining: 36.6s\n849:\tlearn: 11.2020385\ttotal: 3m 25s\tremaining: 36.3s\n850:\tlearn: 11.1649400\ttotal: 3m 26s\tremaining: 36.1s\n851:\tlearn: 11.1363654\ttotal: 3m 26s\tremaining: 35.8s\n852:\tlearn: 11.1011641\ttotal: 3m 26s\tremaining: 35.6s\n853:\tlearn: 11.0779951\ttotal: 3m 26s\tremaining: 35.4s\n854:\tlearn: 11.0544317\ttotal: 3m 27s\tremaining: 35.1s\n855:\tlearn: 11.0325077\ttotal: 3m 27s\tremaining: 34.9s\n856:\tlearn: 11.0081563\ttotal: 3m 27s\tremaining: 34.6s\n857:\tlearn: 10.9822925\ttotal: 3m 27s\tremaining: 34.4s\n858:\tlearn: 10.9621318\ttotal: 3m 27s\tremaining: 34.1s\n859:\tlearn: 10.9230181\ttotal: 3m 28s\tremaining: 33.9s\n860:\tlearn: 10.8980631\ttotal: 3m 28s\tremaining: 33.7s\n861:\tlearn: 10.8783142\ttotal: 3m 28s\tremaining: 33.4s\n862:\tlearn: 10.8637672\ttotal: 3m 28s\tremaining: 33.2s\n863:\tlearn: 10.8427625\ttotal: 3m 29s\tremaining: 32.9s\n864:\tlearn: 10.8085907\ttotal: 3m 29s\tremaining: 32.7s\n865:\tlearn: 10.7844253\ttotal: 3m 29s\tremaining: 32.4s\n866:\tlearn: 10.7746903\ttotal: 3m 29s\tremaining: 32.2s\n867:\tlearn: 10.7590845\ttotal: 3m 30s\tremaining: 31.9s\n868:\tlearn: 10.7181532\ttotal: 3m 30s\tremaining: 31.7s\n869:\tlearn: 10.6945921\ttotal: 3m 30s\tremaining: 31.5s\n870:\tlearn: 10.6758911\ttotal: 3m 30s\tremaining: 31.2s\n871:\tlearn: 10.6542200\ttotal: 3m 31s\tremaining: 31s\n872:\tlearn: 10.6342849\ttotal: 3m 31s\tremaining: 30.7s\n873:\tlearn: 10.6112864\ttotal: 3m 31s\tremaining: 30.5s\n874:\tlearn: 10.5951487\ttotal: 3m 31s\tremaining: 30.3s\n875:\tlearn: 10.5609021\ttotal: 3m 32s\tremaining: 30s\n876:\tlearn: 10.5390555\ttotal: 3m 32s\tremaining: 29.8s\n877:\tlearn: 10.5325391\ttotal: 3m 32s\tremaining: 29.5s\n878:\tlearn: 10.5296286\ttotal: 3m 32s\tremaining: 29.3s\n879:\tlearn: 10.5003723\ttotal: 3m 32s\tremaining: 29s\n880:\tlearn: 10.4858182\ttotal: 3m 33s\tremaining: 28.8s\n881:\tlearn: 10.4696152\ttotal: 3m 33s\tremaining: 28.6s\n882:\tlearn: 10.4488256\ttotal: 3m 33s\tremaining: 28.3s\n883:\tlearn: 10.4447011\ttotal: 3m 33s\tremaining: 28.1s\n884:\tlearn: 10.4132899\ttotal: 3m 34s\tremaining: 27.8s\n885:\tlearn: 10.3857596\ttotal: 3m 34s\tremaining: 27.6s\n886:\tlearn: 10.3513036\ttotal: 3m 34s\tremaining: 27.3s\n887:\tlearn: 10.3448437\ttotal: 3m 34s\tremaining: 27.1s\n888:\tlearn: 10.3207286\ttotal: 3m 35s\tremaining: 26.9s\n889:\tlearn: 10.2863499\ttotal: 3m 35s\tremaining: 26.6s\n890:\tlearn: 10.2524282\ttotal: 3m 35s\tremaining: 26.4s\n891:\tlearn: 10.2242400\ttotal: 3m 35s\tremaining: 26.1s\n892:\tlearn: 10.2050081\ttotal: 3m 36s\tremaining: 25.9s\n893:\tlearn: 10.1938920\ttotal: 3m 36s\tremaining: 25.7s\n894:\tlearn: 10.1768724\ttotal: 3m 36s\tremaining: 25.4s\n895:\tlearn: 10.1517770\ttotal: 3m 36s\tremaining: 25.2s\n896:\tlearn: 10.1291682\ttotal: 3m 37s\tremaining: 24.9s\n897:\tlearn: 10.1117160\ttotal: 3m 37s\tremaining: 24.7s\n898:\tlearn: 10.0899318\ttotal: 3m 37s\tremaining: 24.4s\n899:\tlearn: 10.0551459\ttotal: 3m 37s\tremaining: 24.2s\n900:\tlearn: 10.0306333\ttotal: 3m 38s\tremaining: 24s\n901:\tlearn: 10.0071729\ttotal: 3m 38s\tremaining: 23.7s\n902:\tlearn: 9.9846724\ttotal: 3m 38s\tremaining: 23.5s\n903:\tlearn: 9.9557345\ttotal: 3m 38s\tremaining: 23.2s\n904:\tlearn: 9.9322138\ttotal: 3m 38s\tremaining: 23s\n905:\tlearn: 9.9290331\ttotal: 3m 39s\tremaining: 22.7s\n906:\tlearn: 9.9200049\ttotal: 3m 39s\tremaining: 22.5s\n907:\tlearn: 9.9170463\ttotal: 3m 39s\tremaining: 22.3s\n908:\tlearn: 9.8953721\ttotal: 3m 39s\tremaining: 22s\n909:\tlearn: 9.8820678\ttotal: 3m 40s\tremaining: 21.8s\n910:\tlearn: 9.8530404\ttotal: 3m 40s\tremaining: 21.5s\n911:\tlearn: 9.8259241\ttotal: 3m 40s\tremaining: 21.3s\n912:\tlearn: 9.8208580\ttotal: 3m 40s\tremaining: 21s\n913:\tlearn: 9.8108769\ttotal: 3m 41s\tremaining: 20.8s\n914:\tlearn: 9.7891841\ttotal: 3m 41s\tremaining: 20.6s\n915:\tlearn: 9.7722378\ttotal: 3m 41s\tremaining: 20.3s\n916:\tlearn: 9.7425912\ttotal: 3m 41s\tremaining: 20.1s\n917:\tlearn: 9.7174115\ttotal: 3m 42s\tremaining: 19.8s\n918:\tlearn: 9.7001616\ttotal: 3m 42s\tremaining: 19.6s\n919:\tlearn: 9.6760386\ttotal: 3m 42s\tremaining: 19.4s\n920:\tlearn: 9.6598586\ttotal: 3m 42s\tremaining: 19.1s\n921:\tlearn: 9.6301925\ttotal: 3m 43s\tremaining: 18.9s\n922:\tlearn: 9.6035717\ttotal: 3m 43s\tremaining: 18.6s\n923:\tlearn: 9.5696556\ttotal: 3m 43s\tremaining: 18.4s\n924:\tlearn: 9.5498867\ttotal: 3m 43s\tremaining: 18.1s\n925:\tlearn: 9.5214097\ttotal: 3m 43s\tremaining: 17.9s\n926:\tlearn: 9.4955743\ttotal: 3m 44s\tremaining: 17.7s\n927:\tlearn: 9.4745701\ttotal: 3m 44s\tremaining: 17.4s\n928:\tlearn: 9.4724171\ttotal: 3m 44s\tremaining: 17.2s\n929:\tlearn: 9.4483717\ttotal: 3m 44s\tremaining: 16.9s\n930:\tlearn: 9.4319064\ttotal: 3m 45s\tremaining: 16.7s\n931:\tlearn: 9.4259988\ttotal: 3m 45s\tremaining: 16.4s\n932:\tlearn: 9.4205774\ttotal: 3m 45s\tremaining: 16.2s\n933:\tlearn: 9.4014815\ttotal: 3m 45s\tremaining: 16s\n934:\tlearn: 9.3836587\ttotal: 3m 46s\tremaining: 15.7s\n935:\tlearn: 9.3549776\ttotal: 3m 46s\tremaining: 15.5s\n936:\tlearn: 9.3248977\ttotal: 3m 46s\tremaining: 15.2s\n937:\tlearn: 9.3109028\ttotal: 3m 46s\tremaining: 15s\n938:\tlearn: 9.2837549\ttotal: 3m 47s\tremaining: 14.8s\n939:\tlearn: 9.2513617\ttotal: 3m 47s\tremaining: 14.5s\n940:\tlearn: 9.2292836\ttotal: 3m 47s\tremaining: 14.3s\n941:\tlearn: 9.2039973\ttotal: 3m 47s\tremaining: 14s\n942:\tlearn: 9.1701714\ttotal: 3m 48s\tremaining: 13.8s\n943:\tlearn: 9.1470420\ttotal: 3m 48s\tremaining: 13.5s\n944:\tlearn: 9.1447060\ttotal: 3m 48s\tremaining: 13.3s\n945:\tlearn: 9.1188233\ttotal: 3m 48s\tremaining: 13.1s\n946:\tlearn: 9.1157437\ttotal: 3m 49s\tremaining: 12.8s\n947:\tlearn: 9.0938372\ttotal: 3m 49s\tremaining: 12.6s\n948:\tlearn: 9.0733551\ttotal: 3m 49s\tremaining: 12.3s\n949:\tlearn: 9.0404139\ttotal: 3m 49s\tremaining: 12.1s\n950:\tlearn: 9.0171885\ttotal: 3m 50s\tremaining: 11.8s\n951:\tlearn: 8.9942090\ttotal: 3m 50s\tremaining: 11.6s\n952:\tlearn: 8.9867521\ttotal: 3m 50s\tremaining: 11.4s\n953:\tlearn: 8.9656940\ttotal: 3m 50s\tremaining: 11.1s\n954:\tlearn: 8.9358164\ttotal: 3m 50s\tremaining: 10.9s\n955:\tlearn: 8.9165064\ttotal: 3m 51s\tremaining: 10.6s\n956:\tlearn: 8.9004036\ttotal: 3m 51s\tremaining: 10.4s\n957:\tlearn: 8.8793264\ttotal: 3m 51s\tremaining: 10.2s\n958:\tlearn: 8.8602115\ttotal: 3m 51s\tremaining: 9.91s\n959:\tlearn: 8.8343748\ttotal: 3m 52s\tremaining: 9.67s\n960:\tlearn: 8.8169735\ttotal: 3m 52s\tremaining: 9.43s\n961:\tlearn: 8.7999366\ttotal: 3m 52s\tremaining: 9.19s\n962:\tlearn: 8.7696006\ttotal: 3m 52s\tremaining: 8.95s\n963:\tlearn: 8.7534495\ttotal: 3m 53s\tremaining: 8.7s\n964:\tlearn: 8.7345422\ttotal: 3m 53s\tremaining: 8.46s\n965:\tlearn: 8.7164593\ttotal: 3m 53s\tremaining: 8.22s\n966:\tlearn: 8.6985552\ttotal: 3m 53s\tremaining: 7.98s\n967:\tlearn: 8.6615228\ttotal: 3m 54s\tremaining: 7.74s\n968:\tlearn: 8.6380331\ttotal: 3m 54s\tremaining: 7.5s\n969:\tlearn: 8.6087224\ttotal: 3m 54s\tremaining: 7.26s\n970:\tlearn: 8.5815141\ttotal: 3m 54s\tremaining: 7.02s\n971:\tlearn: 8.5619248\ttotal: 3m 55s\tremaining: 6.78s\n972:\tlearn: 8.5434789\ttotal: 3m 55s\tremaining: 6.54s\n973:\tlearn: 8.5184021\ttotal: 3m 55s\tremaining: 6.29s\n974:\tlearn: 8.5040781\ttotal: 3m 56s\tremaining: 6.05s\n975:\tlearn: 8.4847327\ttotal: 3m 56s\tremaining: 5.81s\n976:\tlearn: 8.4594168\ttotal: 3m 56s\tremaining: 5.57s\n977:\tlearn: 8.4386279\ttotal: 3m 56s\tremaining: 5.33s\n978:\tlearn: 8.4218067\ttotal: 3m 57s\tremaining: 5.08s\n979:\tlearn: 8.4164580\ttotal: 3m 57s\tremaining: 4.84s\n980:\tlearn: 8.3945561\ttotal: 3m 57s\tremaining: 4.6s\n981:\tlearn: 8.3843612\ttotal: 3m 57s\tremaining: 4.36s\n982:\tlearn: 8.3595680\ttotal: 3m 57s\tremaining: 4.12s\n983:\tlearn: 8.3340069\ttotal: 3m 58s\tremaining: 3.87s\n984:\tlearn: 8.3281350\ttotal: 3m 58s\tremaining: 3.63s\n985:\tlearn: 8.3070122\ttotal: 3m 58s\tremaining: 3.39s\n986:\tlearn: 8.2851188\ttotal: 3m 58s\tremaining: 3.15s\n987:\tlearn: 8.2510225\ttotal: 3m 59s\tremaining: 2.9s\n988:\tlearn: 8.2259320\ttotal: 3m 59s\tremaining: 2.66s\n989:\tlearn: 8.1984606\ttotal: 3m 59s\tremaining: 2.42s\n990:\tlearn: 8.1829152\ttotal: 3m 59s\tremaining: 2.18s\n991:\tlearn: 8.1800080\ttotal: 4m\tremaining: 1.94s\n992:\tlearn: 8.1496263\ttotal: 4m\tremaining: 1.69s\n993:\tlearn: 8.1239239\ttotal: 4m\tremaining: 1.45s\n994:\tlearn: 8.0989824\ttotal: 4m\tremaining: 1.21s\n995:\tlearn: 8.0726533\ttotal: 4m 1s\tremaining: 968ms\n996:\tlearn: 8.0551053\ttotal: 4m 1s\tremaining: 726ms\n997:\tlearn: 8.0481978\ttotal: 4m 1s\tremaining: 484ms\n998:\tlearn: 8.0232159\ttotal: 4m 1s\tremaining: 242ms\n999:\tlearn: 7.9954265\ttotal: 4m 2s\tremaining: 0us\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ind = y_train_pred_outliers[y_train_pred_outliers['predicted']==1].index\n",
    "y_train_pred.loc[ind] = 1093\n",
    "\n",
    "ind = y_val_pred_outliers[y_val_pred_outliers['predicted']==1].index\n",
    "y_val_pred.loc[ind] = 1093\n",
    "\n",
    "mae_train = round(mean_absolute_error(y_train, y_train_pred), 2)\n",
    "mae_val = round(mean_absolute_error(y_val, y_val_pred), 2)\n",
    "print(f'MAE on train set = {mae_train}')\n",
    "print(f'MAE on test set = {mae_val}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T09:15:50.052096Z",
     "iopub.execute_input": "2023-08-20T09:15:50.052485Z",
     "iopub.status.idle": "2023-08-20T09:15:50.067457Z",
     "shell.execute_reply.started": "2023-08-20T09:15:50.052457Z",
     "shell.execute_reply": "2023-08-20T09:15:50.065364Z"
    },
    "trusted": true
   },
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "text": "MAE on train set = 11.63\nMAE on test set = 77.67\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Better than linear regression but worse than previous gradboosting regression. Previous metrics:\n",
    "- MAE on train set = 37.2\n",
    "- MAE on test set = 91.37\n",
    "\n",
    "We still need to deal with overfitting but for the last algorithm overfitting is much less. So, in total the last algorithm is much better."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.hist(y_val_pred, bins=30, alpha=0.6, label='Predicted values on the test set')\n",
    "plt.hist(y_val, bins=30, alpha=0.6, label='True values of the test set')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-20T09:16:04.496751Z",
     "iopub.execute_input": "2023-08-20T09:16:04.497190Z",
     "iopub.status.idle": "2023-08-20T09:16:04.806141Z",
     "shell.execute_reply.started": "2023-08-20T09:16:04.497160Z",
     "shell.execute_reply": "2023-08-20T09:16:04.804857Z"
    },
    "trusted": true
   },
   "execution_count": 50,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 600x300 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {}
    }
   ]
  }
 ]
}
